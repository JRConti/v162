---
title: 'Flowformer: Linearizing Transformers with Conservation Flows'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'Transformers based on the attention mechanism have achieved impressive
  success in various areas. However, the attention mechanism has a quadratic complexity,
  significantly impeding Transformers from dealing with numerous tokens and scaling
  up to bigger models. Previous methods mainly utilize the similarity decomposition
  and the associativity of matrix multiplication to devise linear-time attention mechanisms.
  They avoid degeneration of attention to a trivial distribution by reintroducing
  inductive biases such as the locality, thereby at the expense of model generality
  and expressiveness. In this paper, we linearize Transformers free from specific
  inductive biases based on the flow network theory. We cast attention as the information
  flow aggregated from the sources (values) to the sinks (results) through the learned
  flow capacities (attentions). Within this framework, we apply the property of flow
  conservation into attention and propose the Flow-Attention mechanism of linear complexity.
  By respectively conserving the incoming flow of sinks for source competition and
  the outgoing flow of sources for sink allocation, Flow-Attention inherently generates
  informative attentions without using specific inductive biases. Empowered by the
  Flow-Attention, Flowformer yields strong performance in linear time for wide areas,
  including long sequence, time series, vision, natural language, and reinforcement
  learning. The code and settings are available at this repository: https://github.com/thuml/Flowformer.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu22m
month: 0
tex_title: 'Flowformer: Linearizing Transformers with Conservation Flows'
firstpage: 24226
lastpage: 24242
page: 24226-24242
order: 24226
cycles: false
bibtex_author: Wu, Haixu and Wu, Jialong and Xu, Jiehui and Wang, Jianmin and Long,
  Mingsheng
author:
- given: Haixu
  family: Wu
- given: Jialong
  family: Wu
- given: Jiehui
  family: Xu
- given: Jianmin
  family: Wang
- given: Mingsheng
  family: Long
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/wu22m/wu22m.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
