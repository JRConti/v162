---
title: Implicit Bias of the Step Size in Linear Diagonal Neural Networks
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Focusing on diagonal linear networks as a model for understanding the implicit
  bias in underdetermined models, we show how the gradient descent step size can have
  a large qualitative effect on the implicit bias, and thus on generalization ability.
  In particular, we show how using large step size for non-centered data can change
  the implicit bias from a "kernel" type behavior to a "rich" (sparsity-inducing)
  regime â€” even when gradient flow, studied in previous works, would not escape the
  "kernel" regime. We do so by using dynamic stability, proving that convergence to
  dynamically stable global minima entails a bound on some weighted $\ell_1$-norm
  of the linear predictor, i.e. a "rich" regime. We prove this leads to good generalization
  in a sparse regression setting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nacson22a
month: 0
tex_title: Implicit Bias of the Step Size in Linear Diagonal Neural Networks
firstpage: 16270
lastpage: 16295
page: 16270-16295
order: 16270
cycles: false
bibtex_author: Nacson, Mor Shpigel and Ravichandran, Kavya and Srebro, Nathan and
  Soudry, Daniel
author:
- given: Mor Shpigel
  family: Nacson
- given: Kavya
  family: Ravichandran
- given: Nathan
  family: Srebro
- given: Daniel
  family: Soudry
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/nacson22a/nacson22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
