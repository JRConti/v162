---
title: 'Linearity Grafting: Relaxed Neuron Pruning Helps Certifiable Robustness'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Certifiable robustness is a highly desirable property for adopting deep
  neural networks (DNNs) in safety-critical scenarios, but often demands tedious computations
  to establish. The main hurdle lies in the massive amount of non-linearity in large
  DNNs. To trade off the DNN expressiveness (which calls for more non-linearity) and
  robustness certification scalability (which prefers more linearity), we propose
  a novel solution to strategically manipulate neurons, by "grafting" appropriate
  levels of linearity. The core of our proposal is to first linearize insignificant
  ReLU neurons, to eliminate the non-linear components that are both redundant for
  DNN performance and harmful to its certification. We then optimize the associated
  slopes and intercepts of the replaced linear activations for restoring model performance
  while maintaining certifiability. Hence, typical neuron pruning could be viewed
  as a special case of grafting a linear function of the fixed zero slopes and intercept,
  that might overly restrict the network flexibility and sacrifice its performance.
  Extensive experiments on multiple datasets and network backbones show that our linearity
  grafting can (1) effectively tighten certified bounds; (2) achieve competitive certifiable
  robustness without certified robust training (i.e., over 30% improvements on CIFAR-10
  models); and (3) scale up complete verification to large adversarially trained models
  with 17M parameters. Codes are available at https://github.com/VITA-Group/Linearity-Grafting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen22af
month: 0
tex_title: 'Linearity Grafting: Relaxed Neuron Pruning Helps Certifiable Robustness'
firstpage: 3760
lastpage: 3772
page: 3760-3772
order: 3760
cycles: false
bibtex_author: Chen, Tianlong and Zhang, Huan and Zhang, Zhenyu and Chang, Shiyu and
  Liu, Sijia and Chen, Pin-Yu and Wang, Zhangyang
author:
- given: Tianlong
  family: Chen
- given: Huan
  family: Zhang
- given: Zhenyu
  family: Zhang
- given: Shiyu
  family: Chang
- given: Sijia
  family: Liu
- given: Pin-Yu
  family: Chen
- given: Zhangyang
  family: Wang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/chen22af/chen22af.pdf
extras:
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2022/other_files/chen22af-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
