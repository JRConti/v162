---
title: What Dense Graph Do You Need for Self-Attention?
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Transformers have made progress in miscellaneous tasks, but suffer from
  quadratic computational and memory complexities. Recent works propose sparse transformers
  with attention on sparse graphs to reduce complexity and remain strong performance.
  While effective, the crucial parts of how dense a graph needs to be to perform well
  are not fully explored. In this paper, we propose Normalized Information Payload
  (NIP), a graph scoring function measuring information transfer on graph, which provides
  an analysis tool for trade-offs between performance and complexity. Guided by this
  theoretical analysis, we present Hypercube Transformer, a sparse transformer that
  models token interactions in a hypercube and shows comparable or even better results
  with vanilla transformer while yielding $O(N\log N)$ complexity with sequence length
  $N$. Experiments on tasks requiring various sequence lengths lay validation for
  our graph function well.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang22l
month: 0
tex_title: What Dense Graph Do You Need for Self-Attention?
firstpage: 22752
lastpage: 22768
page: 22752-22768
order: 22752
cycles: false
bibtex_author: Wang, Yuxin and Lee, Chu-Tak and Guo, Qipeng and Yin, Zhangyue and
  Zhou, Yunhua and Huang, Xuanjing and Qiu, Xipeng
author:
- given: Yuxin
  family: Wang
- given: Chu-Tak
  family: Lee
- given: Qipeng
  family: Guo
- given: Zhangyue
  family: Yin
- given: Yunhua
  family: Zhou
- given: Xuanjing
  family: Huang
- given: Xipeng
  family: Qiu
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/wang22l/wang22l.pdf
extras:
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2022/other_files/wang22l-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
