---
title: 'DAdaQuant: Doubly-adaptive quantization for communication-efficient Federated
  Learning'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Federated Learning (FL) is a powerful technique to train a model on a server
  with data from several clients in a privacy-preserving manner. FL incurs significant
  communication costs because it repeatedly transmits the model between the server
  and clients. Recently proposed algorithms quantize the model parameters to efficiently
  compress FL communication. We find that dynamic adaptations of the quantization
  level can boost compression without sacrificing model quality. We introduce DAdaQuant
  as a doubly-adaptive quantization algorithm that dynamically changes the quantization
  level across time and different clients. Our experiments show that DAdaQuant consistently
  improves client$\rightarrow$server compression, outperforming the strongest non-adaptive
  baselines by up to $2.8\times$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: honig22a
month: 0
tex_title: "{DA}da{Q}uant: Doubly-adaptive quantization for communication-efficient
  Federated Learning"
firstpage: 8852
lastpage: 8866
page: 8852-8866
order: 8852
cycles: false
bibtex_author: H{\"o}nig, Robert and Zhao, Yiren and Mullins, Robert
author:
- given: Robert
  family: HÃ¶nig
- given: Yiren
  family: Zhao
- given: Robert
  family: Mullins
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/honig22a/honig22a.pdf
extras:
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2022/other_files/honig22a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
