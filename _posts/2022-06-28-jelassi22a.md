---
title: Towards understanding how momentum improves generalization in deep learning
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Stochastic gradient descent (SGD) with momentum is widely used for training
  modern deep learning architectures. While it is well-understood that using momentum
  can lead to faster convergence rate in various settings, it has also been observed
  that momentum yields higher generalization. Prior work argue that momentum stabilizes
  the SGD noise during training and this leads to higher generalization. In this paper,
  we adopt another perspective and first empirically show that gradient descent with
  momentum (GD+M) significantly improves generalization compared to gradient descent
  (GD) in some deep learning problems. From this observation, we formally study how
  momentum improves generalization. We devise a binary classification setting where
  a one-hidden layer (over-parameterized) convolutional neural network trained with
  GD+M provably generalizes better than the same network trained with GD, when both
  algorithms are similarly initialized. The key insight in our analysis is that momentum
  is beneficial in datasets where the examples share some feature but differ in their
  margin. Contrary to GD that memorizes the small margin data, GD+M still learns the
  feature in these data thanks to its historical gradients. Lastly, we empirically
  validate our theoretical findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jelassi22a
month: 0
tex_title: Towards understanding how momentum improves generalization in deep learning
firstpage: 9965
lastpage: 10040
page: 9965-10040
order: 9965
cycles: false
bibtex_author: Jelassi, Samy and Li, Yuanzhi
author:
- given: Samy
  family: Jelassi
- given: Yuanzhi
  family: Li
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/jelassi22a/jelassi22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
