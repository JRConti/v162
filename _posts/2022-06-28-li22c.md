---
title: Spatial-Channel Token Distillation for Vision MLPs
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Recently, neural architectures with all Multi-layer Perceptrons (MLPs) have
  attracted great research interest from the computer vision community. However, the
  inefficient mixing of spatial-channel information causes MLP-like vision models
  to demand tremendous pre-training on large-scale datasets. This work solves the
  problem from a novel knowledge distillation perspective. We propose a novel Spatial-channel
  Token Distillation (STD) method, which improves the information mixing in the two
  dimensions by introducing distillation tokens to each of them. A mutual information
  regularization is further introduced to let distillation tokens focus on their specific
  dimensions and maximize the performance gain. Extensive experiments on ImageNet
  for several MLP-like architectures demonstrate that the proposed token distillation
  mechanism can efficiently improve the accuracy. For example, the proposed STD boosts
  the top-1 accuracy of Mixer-S16 on ImageNet from 73.8% to 75.7% without any costly
  pre-training on JFT-300M. When applied to stronger architectures, e.g. CycleMLP-B1
  and CycleMLP-B2, STD can still harvest about 1.1% and 0.5% accuracy gains, respectively.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li22c
month: 0
tex_title: Spatial-Channel Token Distillation for Vision {MLP}s
firstpage: 12685
lastpage: 12695
page: 12685-12695
order: 12685
cycles: false
bibtex_author: Li, Yanxi and Chen, Xinghao and Dong, Minjing and Tang, Yehui and Wang,
  Yunhe and Xu, Chang
author:
- given: Yanxi
  family: Li
- given: Xinghao
  family: Chen
- given: Minjing
  family: Dong
- given: Yehui
  family: Tang
- given: Yunhe
  family: Wang
- given: Chang
  family: Xu
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/li22c/li22c.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
