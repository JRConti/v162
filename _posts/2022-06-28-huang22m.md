---
title: Directed Acyclic Transformer for Non-Autoregressive Machine Translation
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Non-autoregressive Transformers (NATs) significantly reduce the decoding
  latency by generating all tokens in parallel. However, such independent predictions
  prevent NATs from capturing the dependencies between the tokens for generating multiple
  possible translations. In this paper, we propose Directed Acyclic Transfomer (DA-Transformer),
  which represents the hidden states in a Directed Acyclic Graph (DAG), where each
  path of the DAG corresponds to a specific translation. The whole DAG simultaneously
  captures multiple translations and facilitates fast predictions in a non-autoregressive
  fashion. Experiments on the raw training data of WMT benchmark show that DA-Transformer
  substantially outperforms previous NATs by about 3 BLEU on average, which is the
  first NAT model that achieves competitive results with autoregressive Transformers
  without relying on knowledge distillation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang22m
month: 0
tex_title: Directed Acyclic Transformer for Non-Autoregressive Machine Translation
firstpage: 9410
lastpage: 9428
page: 9410-9428
order: 9410
cycles: false
bibtex_author: Huang, Fei and Zhou, Hao and Liu, Yang and Li, Hang and Huang, Minlie
author:
- given: Fei
  family: Huang
- given: Hao
  family: Zhou
- given: Yang
  family: Liu
- given: Hang
  family: Li
- given: Minlie
  family: Huang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/huang22m/huang22m.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
