---
title: Understanding The Robustness in Vision Transformers
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'Recent studies show that Vision Transformers (ViTs) exhibit strong robustness
  against various corruptions. Although this property is partly attributed to the
  self-attention mechanism, there is still a lack of an explanatory framework towards
  a more systematic understanding. In this paper, we examine the role of self-attention
  in learning robust representations. Our study is motivated by the intriguing properties
  of self-attention in visual grouping which indicate that self-attention could promote
  improved mid-level representation and robustness. We thus propose a family of fully
  attentional networks (FANs) that incorporate self-attention in both token mixing
  and channel processing. We validate the design comprehensively on various hierarchical
  backbones. Our model with a DeiT architecture achieves a state-of-the-art 47.6%
  mCE on ImageNet-C with 29M parameters. We also demonstrate significantly improved
  robustness in two downstream tasks: semantic segmentation and object detection'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou22m
month: 0
tex_title: Understanding The Robustness in Vision Transformers
firstpage: 27378
lastpage: 27394
page: 27378-27394
order: 27378
cycles: false
bibtex_author: Zhou, Daquan and Yu, Zhiding and Xie, Enze and Xiao, Chaowei and Anandkumar,
  Animashree and Feng, Jiashi and Alvarez, Jose M.
author:
- given: Daquan
  family: Zhou
- given: Zhiding
  family: Yu
- given: Enze
  family: Xie
- given: Chaowei
  family: Xiao
- given: Animashree
  family: Anandkumar
- given: Jiashi
  family: Feng
- given: Jose M.
  family: Alvarez
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/zhou22m/zhou22m.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
