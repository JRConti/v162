---
title: Multicoated Supermasks Enhance Hidden Networks
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Hidden Networks (Ramanujan et al., 2020) showed the possibility of finding
  accurate subnetworks within a randomly weighted neural network by training a connectivity
  mask, referred to as supermask. We show that the supermask stops improving even
  though gradients are not zero, thus underutilizing backpropagated information. To
  address this we propose a method that extends Hidden Networks by training an overlay
  of multiple hierarchical supermasks{—}a multicoated supermask. This method shows
  that using multiple supermasks for a single task achieves higher accuracy without
  additional training cost. Experiments on CIFAR-10 and ImageNet show that Multicoated
  Supermasks enhance the tradeoff between accuracy and model size. A ResNet-101 using
  a 7-coated supermask outperforms its Hidden Networks counterpart by 4%, matching
  the accuracy of a dense ResNet-50 while being an order of magnitude smaller.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: okoshi22a
month: 0
tex_title: Multicoated Supermasks Enhance Hidden Networks
firstpage: 17045
lastpage: 17055
page: 17045-17055
order: 17045
cycles: false
bibtex_author: Okoshi, Yasuyuki and Garc\'{\i}a-Arias, \'Angel L{\'o}pez and Hirose,
  Kazutoshi and Ando, Kota and Kawamura, Kazushi and Van Chu, Thiem and Motomura,
  Masato and Yu, Jaehoon
author:
- given: Yasuyuki
  family: Okoshi
- given: Ángel López
  family: Garcı́a-Arias
- given: Kazutoshi
  family: Hirose
- given: Kota
  family: Ando
- given: Kazushi
  family: Kawamura
- given: Thiem
  family: Van Chu
- given: Masato
  family: Motomura
- given: Jaehoon
  family: Yu
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/okoshi22a/okoshi22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
