---
title: Training Discrete Deep Generative Models via Gapped Straight-Through Estimator
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: While deep generative models have succeeded in image processing, natural
  language processing, and reinforcement learning, training that involves discrete
  random variables remains challenging due to the high variance of its gradient estimation
  process. Monte Carlo is a common solution used in most variance reduction approaches.
  However, this involves time-consuming resampling and multiple function evaluations.
  We propose a Gapped Straight-Through (GST) estimator to reduce the variance without
  incurring resampling overhead. This estimator is inspired by the essential properties
  of Straight-Through Gumbel-Softmax. We determine these properties and show via an
  ablation study that they are essential. Experiments demonstrate that the proposed
  GST estimator enjoys better performance compared to strong baselines on two discrete
  deep generative modeling tasks, MNIST-VAE and ListOps.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fan22a
month: 0
tex_title: Training Discrete Deep Generative Models via Gapped Straight-Through Estimator
firstpage: 6059
lastpage: 6073
page: 6059-6073
order: 6059
cycles: false
bibtex_author: Fan, Ting-Han and Chi, Ta-Chung and Rudnicky, Alexander I. and Ramadge,
  Peter J
author:
- given: Ting-Han
  family: Fan
- given: Ta-Chung
  family: Chi
- given: Alexander I.
  family: Rudnicky
- given: Peter J
  family: Ramadge
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/fan22a/fan22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
