---
title: 'HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot
  Learning'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: In this work we propose a HyperTransformer, a Transformer-based model for
  supervised and semi-supervised few-shot learning that generates weights of a convolutional
  neural network (CNN) directly from support samples. Since the dependence of a small
  generated CNN model on a specific task is encoded by a high-capacity Transformer
  model, we effectively decouple the complexity of the large task space from the complexity
  of individual tasks. Our method is particularly effective for small target CNN architectures
  where learning a fixed universal task-independent embedding is not optimal and better
  performance is attained when the information about the task can modulate all model
  parameters. For larger models we discover that generating the last layer alone allows
  us to produce competitive or better results than those obtained with state-of-the-art
  methods while being end-to-end differentiable.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhmoginov22a
month: 0
tex_title: "{H}yper{T}ransformer: Model Generation for Supervised and Semi-Supervised
  Few-Shot Learning"
firstpage: 27075
lastpage: 27098
page: 27075-27098
order: 27075
cycles: false
bibtex_author: Zhmoginov, Andrey and Sandler, Mark and Vladymyrov, Maksym
author:
- given: Andrey
  family: Zhmoginov
- given: Mark
  family: Sandler
- given: Maksym
  family: Vladymyrov
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/zhmoginov22a/zhmoginov22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
