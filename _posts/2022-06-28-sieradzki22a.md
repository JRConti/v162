---
title: Coin Flipping Neural Networks
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: We show that neural networks with access to randomness can outperform deterministic
  networks by using amplification. We call such networks Coin-Flipping Neural Networks,
  or CFNNs. We show that a CFNN can approximate the indicator of a d-dimensional ball
  to arbitrary accuracy with only 2 layers and O(1) neurons, where a 2-layer deterministic
  network was shown to require Omega(e^d) neurons, an exponential improvement. We
  prove a highly non-trivial result, that for almost any classification problem, there
  exists a trivially simple network that solves it given a sufficiently powerful generator
  for the networkâ€™s weights. Combining these results we conjecture that for most classification
  problems, there is a CFNN which solves them with higher accuracy or fewer neurons
  than any deterministic network. Finally, we verify our proofs experimentally using
  novel CFNN architectures on CIFAR10 and CIFAR100, reaching an improvement of 9.25%
  from the baseline.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sieradzki22a
month: 0
tex_title: Coin Flipping Neural Networks
firstpage: 20195
lastpage: 20214
page: 20195-20214
order: 20195
cycles: false
bibtex_author: Sieradzki, Yuval and Hodos, Nitzan and Yehuda, Gal and Schuster, Assaf
author:
- given: Yuval
  family: Sieradzki
- given: Nitzan
  family: Hodos
- given: Gal
  family: Yehuda
- given: Assaf
  family: Schuster
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/sieradzki22a/sieradzki22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
