---
title: 'Wide Bayesian neural networks have a simple weight posterior: theory and accelerated
  sampling'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: We introduce repriorisation, a data-dependent reparameterisation which transforms
  a Bayesian neural network (BNN) posterior to a distribution whose KL divergence
  to the BNN prior vanishes as layer widths grow. The repriorisation map acts directly
  on parameters, and its analytic simplicity complements the known neural network
  Gaussian process (NNGP) behaviour of wide BNNs in function space. Exploiting the
  repriorisation, we develop a Markov chain Monte Carlo (MCMC) posterior sampling
  algorithm which mixes faster the wider the BNN. This contrasts with the typically
  poor performance of MCMC in high dimensions. We observe up to 50x higher effective
  sample size relative to no reparametrisation for both fully-connected and residual
  networks. Improvements are achieved at all widths, with the margin between reparametrised
  and standard BNNs growing with layer width.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hron22a
month: 0
tex_title: 'Wide {B}ayesian neural networks have a simple weight posterior: theory
  and accelerated sampling'
firstpage: 8926
lastpage: 8945
page: 8926-8945
order: 8926
cycles: false
bibtex_author: Hron, Jiri and Novak, Roman and Pennington, Jeffrey and Sohl-Dickstein,
  Jascha
author:
- given: Jiri
  family: Hron
- given: Roman
  family: Novak
- given: Jeffrey
  family: Pennington
- given: Jascha
  family: Sohl-Dickstein
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/hron22a/hron22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
