---
title: Generative Modeling for Multi-task Visual Learning
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Generative modeling has recently shown great promise in computer vision,
  but it has mostly focused on synthesizing visually realistic images. In this paper,
  motivated by multi-task learning of shareable feature representations, we consider
  a novel problem of learning a shared generative model that is useful across various
  visual perception tasks. Correspondingly, we propose a general multi-task oriented
  generative modeling (MGM) framework, by coupling a discriminative multi-task network
  with a generative network. While it is challenging to synthesize both RGB images
  and pixel-level annotations in multi-task scenarios, our framework enables us to
  use synthesized images paired with only weak annotations (i.e., image-level scene
  labels) to facilitate multiple visual tasks. Experimental evaluation on challenging
  multi-task benchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM
  framework improves the performance of all the tasks by large margins, consistently
  outperforming state-of-the-art multi-task approaches in different sample-size regimes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bao22c
month: 0
tex_title: Generative Modeling for Multi-task Visual Learning
firstpage: 1537
lastpage: 1554
page: 1537-1554
order: 1537
cycles: false
bibtex_author: Bao, Zhipeng and Hebert, Martial and Wang, Yu-Xiong
author:
- given: Zhipeng
  family: Bao
- given: Martial
  family: Hebert
- given: Yu-Xiong
  family: Wang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/bao22c/bao22c.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
