---
title: 'Accelerated Gradient Methods for Geodesically Convex Optimization: Tractable
  Algorithms and Convergence Analysis'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: We propose computationally tractable accelerated first-order methods for
  Riemannian optimization, extending the Nesterov accelerated gradient (NAG) method.
  For both geodesically convex and geodesically strongly convex objective functions,
  our algorithms are shown to have the same iteration complexities as those for the
  NAG method on Euclidean spaces, under only standard assumptions. To the best of
  our knowledge, the proposed scheme is the first fully accelerated method for geodesically
  convex optimization problems. Our convergence analysis makes use of novel metric
  distortion lemmas as well as carefully designed potential functions. A connection
  with the continuous-time dynamics for modeling Riemannian acceleration in (Alimisis
  et al., 2020) is also identified by letting the stepsize tend to zero. We validate
  our theoretical results through numerical experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim22k
month: 0
tex_title: 'Accelerated Gradient Methods for Geodesically Convex Optimization: Tractable
  Algorithms and Convergence Analysis'
firstpage: 11255
lastpage: 11282
page: 11255-11282
order: 11255
cycles: false
bibtex_author: Kim, Jungbin and Yang, Insoon
author:
- given: Jungbin
  family: Kim
- given: Insoon
  family: Yang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/kim22k/kim22k.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
