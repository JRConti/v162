---
title: 'Restarted Nonconvex Accelerated Gradient Descent: No More Polylogarithmic
  Factor in the $O(ε^-7/4)$ Complexity'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: This paper studies the accelerated gradient descent for general nonconvex
  problems under the gradient Lipschitz and Hessian Lipschitz assumptions. We establish
  that a simple restarted accelerated gradient descent (AGD) finds an $\epsilon$-approximate
  first-order stationary point in $O(\epsilon^{-7/4})$ gradient computations with
  simple proofs. Our complexity does not hide any polylogarithmic factors, and thus
  it improves over the state-of-the-art one by the $O(\log\frac{1}{\epsilon})$ factor.
  Our simple algorithm only consists of Nesterov’s classical AGD and a restart mechanism,
  and it does not need the negative curvature exploitation or the optimization of
  regularized surrogate functions. Technically, our simple proof does not invoke the
  analysis for the strongly convex AGD, which is crucial to remove the $O(\log\frac{1}{\epsilon})$
  factor.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li22o
month: 0
tex_title: 'Restarted Nonconvex Accelerated Gradient Descent: No More Polylogarithmic
  Factor in the $O(ε^{-7/4})$ Complexity'
firstpage: 12901
lastpage: 12916
page: 12901-12916
order: 12901
cycles: false
bibtex_author: Li, Huan and Lin, Zhouchen
author:
- given: Huan
  family: Li
- given: Zhouchen
  family: Lin
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/li22o/li22o.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
