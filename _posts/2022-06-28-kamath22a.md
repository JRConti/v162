---
title: Improved Rates for Differentially Private Stochastic Convex Optimization with
  Heavy-Tailed Data
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: We study stochastic convex optimization with heavy-tailed data under the
  constraint of differential privacy (DP). Most prior work on this problem is restricted
  to the case where the loss function is Lipschitz. Instead, as introduced by Wang,
  Xiao, Devadas, and XuÂ \cite{WangXDX20}, we study general convex loss functions with
  the assumption that the distribution of gradients has bounded $k$-th moments. We
  provide improved upper bounds on the excess population risk under concentrated DP
  for convex and strongly convex loss functions. Along the way, we derive new algorithms
  for private mean estimation of heavy-tailed distributions, under both pure and concentrated
  DP. Finally, we prove nearly-matching lower bounds for private stochastic convex
  optimization with strongly convex losses and mean estimation, showing new separations
  between pure and concentrated DP.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kamath22a
month: 0
tex_title: Improved Rates for Differentially Private Stochastic Convex Optimization
  with Heavy-Tailed Data
firstpage: 10633
lastpage: 10660
page: 10633-10660
order: 10633
cycles: false
bibtex_author: Kamath, Gautam and Liu, Xingtu and Zhang, Huanyu
author:
- given: Gautam
  family: Kamath
- given: Xingtu
  family: Liu
- given: Huanyu
  family: Zhang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/kamath22a/kamath22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
