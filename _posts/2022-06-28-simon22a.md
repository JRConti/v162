---
title: Reverse Engineering the Neural Tangent Kernel
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: The development of methods to guide the design of neural networks is an
  important open challenge for deep learning theory. As a paradigm for principled
  neural architecture design, we propose the translation of high-performing kernels,
  which are better-understood and amenable to first-principles design, into equivalent
  network architectures, which have superior efficiency, flexibility, and feature
  learning. To this end, we constructively prove that, with just an appropriate choice
  of activation function, any positive-semidefinite dot-product kernel can be realized
  as either the NNGP or neural tangent kernel of a fully-connected neural network
  with only one hidden layer. We verify our construction numerically and demonstrate
  its utility as a design tool for finite fully-connected networks in several experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: simon22a
month: 0
tex_title: Reverse Engineering the Neural Tangent Kernel
firstpage: 20215
lastpage: 20231
page: 20215-20231
order: 20215
cycles: false
bibtex_author: Simon, James Benjamin and Anand, Sajant and Deweese, Mike
author:
- given: James Benjamin
  family: Simon
- given: Sajant
  family: Anand
- given: Mike
  family: Deweese
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/simon22a/simon22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
