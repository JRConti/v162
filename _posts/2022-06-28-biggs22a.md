---
title: Non-Vacuous Generalisation Bounds for Shallow Neural Networks
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: We focus on a specific class of shallow neural networks with a single hidden
  layer, namely those with $L_2$-normalised data and either a sigmoid-shaped Gaussian
  error function (“erf”) activation or a Gaussian Error Linear Unit (GELU) activation.
  For these networks, we derive new generalisation bounds through the PAC-Bayesian
  theory; unlike most existing such bounds they apply to neural networks with deterministic
  rather than randomised parameters. Our bounds are empirically non-vacuous when the
  network is trained with vanilla stochastic gradient descent on MNIST and Fashion-MNIST.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: biggs22a
month: 0
tex_title: Non-Vacuous Generalisation Bounds for Shallow Neural Networks
firstpage: 1963
lastpage: 1981
page: 1963-1981
order: 1963
cycles: false
bibtex_author: Biggs, Felix and Guedj, Benjamin
author:
- given: Felix
  family: Biggs
- given: Benjamin
  family: Guedj
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/biggs22a/biggs22a.pdf
extras:
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2022/other_files/biggs22a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
