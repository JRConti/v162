---
title: 'DRAGONN: Distributed Randomized Approximate Gradients of Neural Networks'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Data-parallel distributed training (DDT) has become the de-facto standard
  for accelerating the training of most deep learning tasks on massively parallel
  hardware. In the DDT paradigm, the communication overhead of gradient synchronization
  is the major efficiency bottleneck. A widely adopted approach to tackle this issue
  is gradient sparsification (GS). However, the current GS methods introduce significant
  new overhead in compressing the gradients, outweighing the communication overhead
  and becoming the new efficiency bottleneck. In this paper, we propose DRAGONN, a
  randomized hashing algorithm for GS in DDT. DRAGONN can significantly reduce the
  compression time by up to 70% compared to state-of-the-art GS approaches, and achieve
  up to 3.52x speedup in total training throughput.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang22aj
month: 0
tex_title: "{DRAGONN}: Distributed Randomized Approximate Gradients of Neural Networks"
firstpage: 23274
lastpage: 23291
page: 23274-23291
order: 23274
cycles: false
bibtex_author: Wang, Zhuang and Xu, Zhaozhuo and Wu, Xinyu and Shrivastava, Anshumali
  and Ng, T. S. Eugene
author:
- given: Zhuang
  family: Wang
- given: Zhaozhuo
  family: Xu
- given: Xinyu
  family: Wu
- given: Anshumali
  family: Shrivastava
- given: T. S. Eugene
  family: Ng
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/wang22aj/wang22aj.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
