---
title: Adaptive Random Walk Gradient Descent for Decentralized Optimization
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: In this paper, we study the adaptive step size random walk gradient descent
  with momentum for decentralized optimization, in which the training samples are
  drawn dependently with each other. We establish theoretical convergence rates of
  the adaptive step size random walk gradient descent with momentum for both convex
  and nonconvex settings. In particular, we prove that adaptive random walk algorithms
  perform as well as the non-adaptive method for dependent data in general cases but
  achieve acceleration when the stochastic gradients are “sparse”. Moreover, we study
  the zeroth-order version of adaptive random walk gradient descent and provide corresponding
  convergence results. All assumptions used in this paper are mild and general, making
  our results applicable to many machine learning problems.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sun22b
month: 0
tex_title: Adaptive Random Walk Gradient Descent for Decentralized Optimization
firstpage: 20790
lastpage: 20809
page: 20790-20809
order: 20790
cycles: false
bibtex_author: Sun, Tao and Li, Dongsheng and Wang, Bao
author:
- given: Tao
  family: Sun
- given: Dongsheng
  family: Li
- given: Bao
  family: Wang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/sun22b/sun22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
