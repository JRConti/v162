---
title: Training Your Sparse Neural Network Better with Any Mask
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'Pruning large neural networks to create high-quality, independently trainable
  sparse masks, which can maintain similar performance to their dense counterparts,
  is very desirable due to the reduced space and time complexity. As research effort
  is focused on increasingly sophisticated pruning methods that leads to sparse subnetworks
  trainable from the scratch, we argue for an orthogonal, under-explored theme: improving
  training techniques for pruned sub-networks, i.e. sparse training. Apart from the
  popular belief that only the quality of sparse masks matters for sparse training,
  in this paper we demonstrate an alternative opportunity: one can carefully customize
  the sparse training techniques to deviate from the default dense network training
  protocols, consisting of introducing â€œghost" neurons and skip connections at the
  early stage of training, and strategically modifying the initialization as well
  as labels. Our new sparse training recipe is generally applicable to improving training
  from scratch with various sparse masks. By adopting our newly curated techniques,
  we demonstrate significant performance gains across various popular datasets (CIFAR-10,
  CIFAR-100, TinyImageNet), architectures (ResNet-18/32/104, Vgg16, MobileNet), and
  sparse mask options (lottery ticket, SNIP/GRASP, SynFlow, or even randomly pruning),
  compared to the default training protocols, especially at high sparsity levels.
  Codes will be publicly available.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jaiswal22a
month: 0
tex_title: Training Your Sparse Neural Network Better with Any Mask
firstpage: 9833
lastpage: 9844
page: 9833-9844
order: 9833
cycles: false
bibtex_author: Jaiswal, Ajay Kumar and Ma, Haoyu and Chen, Tianlong and Ding, Ying
  and Wang, Zhangyang
author:
- given: Ajay Kumar
  family: Jaiswal
- given: Haoyu
  family: Ma
- given: Tianlong
  family: Chen
- given: Ying
  family: Ding
- given: Zhangyang
  family: Wang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/jaiswal22a/jaiswal22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
