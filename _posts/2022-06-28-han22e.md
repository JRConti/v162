---
title: Off-Policy Reinforcement Learning with Delayed Rewards
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: We study deep reinforcement learning (RL) algorithms with delayed rewards.
  In many real-world tasks, instant rewards are often not readily accessible or even
  defined immediately after the agent performs actions. In this work, we first formally
  define the environment with delayed rewards and discuss the challenges raised due
  to the non-Markovian nature of such environments. Then, we introduce a general off-policy
  RL framework with a new Q-function formulation that can handle the delayed rewards
  with theoretical convergence guarantees. For practical tasks with high dimensional
  state spaces, we further introduce the HC-decomposition rule of the Q-function in
  our framework which naturally leads to an approximation scheme that helps boost
  the training efficiency and stability. We finally conduct extensive experiments
  to demonstrate the superior performance of our algorithms over the existing work
  and their variants.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: han22e
month: 0
tex_title: Off-Policy Reinforcement Learning with Delayed Rewards
firstpage: 8280
lastpage: 8303
page: 8280-8303
order: 8280
cycles: false
bibtex_author: Han, Beining and Ren, Zhizhou and Wu, Zuofan and Zhou, Yuan and Peng,
  Jian
author:
- given: Beining
  family: Han
- given: Zhizhou
  family: Ren
- given: Zuofan
  family: Wu
- given: Yuan
  family: Zhou
- given: Jian
  family: Peng
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/han22e/han22e.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
