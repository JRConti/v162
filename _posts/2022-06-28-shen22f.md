---
title: Staged Training for Transformer Language Models
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: The current standard approach to scaling transformer language models trains
  each model size from a different random initialization. As an alternative, we consider
  a staged training setup that begins with a small model and incrementally increases
  the amount of compute used for training by applying a "growth operator" to increase
  the model depth and width. By initializing each stage with the output of the previous
  one, the training process effectively re-uses the compute from prior stages and
  becomes more efficient. Our growth operators each take as input the entire training
  state (including model parameters, optimizer state, learning rate schedule, etc.)
  and output a new training state from which training continues. We identify two important
  properties of these growth operators, namely that they preserve both the loss and
  the “training dynamics” after applying the operator. While the loss-preserving property
  has been discussed previously, to the best of our knowledge this work is the first
  to identify the importance of preserving the training dynamics (the rate of decrease
  of the loss during training). To find the optimal schedule for stages, we use the
  scaling laws from (Kaplan et al., 2020) to find a precise schedule that gives the
  most compute saving by starting a new stage when training efficiency starts decreasing.
  We empirically validate our growth operators and staged training for autoregressive
  language models, showing up to 22% compute savings compared to a strong baseline
  trained from scratch. Our code is available at https://github.com/allenai/staged-training.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shen22f
month: 0
tex_title: Staged Training for Transformer Language Models
firstpage: 19893
lastpage: 19908
page: 19893-19908
order: 19893
cycles: false
bibtex_author: Shen, Sheng and Walsh, Pete and Keutzer, Kurt and Dodge, Jesse and
  Peters, Matthew and Beltagy, Iz
author:
- given: Sheng
  family: Shen
- given: Pete
  family: Walsh
- given: Kurt
  family: Keutzer
- given: Jesse
  family: Dodge
- given: Matthew
  family: Peters
- given: Iz
  family: Beltagy
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/shen22f/shen22f.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
