---
title: Variational nearest neighbor Gaussian process
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Variational approximations to Gaussian processes (GPs) typically use a small
  set of inducing points to form a low-rank approximation to the covariance matrix.
  In this work, we instead exploit a sparse approximation of the precision matrix.
  We propose variational nearest neighbor Gaussian process (VNNGP), which introduces
  a prior that only retains correlations within $K$ nearest-neighboring observations,
  thereby inducing sparse precision structure. Using the variational framework, VNNGPâ€™s
  objective can be factorized over both observations and inducing points, enabling
  stochastic optimization with a time complexity of $O(K^3)$. Hence, we can arbitrarily
  scale the inducing point size, even to the point of putting inducing points at every
  observed location. We compare VNNGP to other scalable GPs through various experiments,
  and demonstrate that VNNGP (1) can dramatically outperform low-rank methods, and
  (2) is less prone to overfitting than other nearest neighbor methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu22h
month: 0
tex_title: Variational nearest neighbor {G}aussian process
firstpage: 24114
lastpage: 24130
page: 24114-24130
order: 24114
cycles: false
bibtex_author: Wu, Luhuan and Pleiss, Geoff and Cunningham, John P
author:
- given: Luhuan
  family: Wu
- given: Geoff
  family: Pleiss
- given: John P
  family: Cunningham
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/wu22h/wu22h.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
