---
title: Scaling-up Diverse Orthogonal Convolutional Networks by a Paraunitary Framework
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Enforcing orthogonality in convolutional neural networks is a remedy for
  gradient vanishing/exploding problems and sensitivity to perturbation. Many previous
  approaches for orthogonal convolutions enforce orthogonality on its flattened kernel,
  which, however, do not lead to the orthogonality of the operation. Some recent approaches
  consider orthogonality for standard convolutional layers and propose specific classes
  of their realizations. In this work, we propose a theoretical framework that establishes
  the equivalence between diverse orthogonal convolutional layers in the spatial domain
  and the paraunitary systems in the spectral domain. Since 1D paraunitary systems
  admit a complete factorization, we can parameterize any separable orthogonal convolution
  as a composition of spatial filters. As a result, our framework endows high expressive
  power to various convolutional layers while maintaining their exact orthogonality.
  Furthermore, our layers are memory and computationally efficient for deep networks
  compared to previous designs. Our versatile framework, for the first time, enables
  the study of architectural designs for deep orthogonal networks, such as choices
  of skip connection, initialization, stride, and dilation. Consequently, we scale
  up orthogonal networks to deep architectures, including ResNet and ShuffleNet, substantially
  outperforming their shallower counterparts. Finally, we show how to construct residual
  flows, a flow-based generative model that requires strict Lipschitzness, using our
  orthogonal networks. Our code will be publicly available at https://github.com/umd-huang-lab/ortho-conv
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: su22a
month: 0
tex_title: Scaling-up Diverse Orthogonal Convolutional Networks by a Paraunitary Framework
firstpage: 20546
lastpage: 20579
page: 20546-20579
order: 20546
cycles: false
bibtex_author: Su, Jiahao and Byeon, Wonmin and Huang, Furong
author:
- given: Jiahao
  family: Su
- given: Wonmin
  family: Byeon
- given: Furong
  family: Huang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/su22a/su22a.pdf
extras:
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2022/other_files/su22a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
