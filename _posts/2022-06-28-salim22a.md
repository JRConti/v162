---
title: A Convergence Theory for SVGD in the Population Limit under Talagrand’s Inequality
  T1
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Stein Variational Gradient Descent (SVGD) is an algorithm for sampling from
  a target density which is known up to a multiplicative constant. Although SVGD is
  a popular algorithm in practice, its theoretical study is limited to a few recent
  works. We study the convergence of SVGD in the population limit, (i.e., with an
  infinite number of particles) to sample from a non-logconcave target distribution
  satisfying Talagrand’s inequality T1. We first establish the convergence of the
  algorithm. Then, we establish a dimension-dependent complexity bound in terms of
  the Kernelized Stein Discrepancy (KSD). Unlike existing works, we do not assume
  that the KSD is bounded along the trajectory of the algorithm. Our approach relies
  on interpreting SVGD as a gradient descent over a space of probability measures.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: salim22a
month: 0
tex_title: A Convergence Theory for {SVGD} in the Population Limit under Talagrand’s
  Inequality T1
firstpage: 19139
lastpage: 19152
page: 19139-19152
order: 19139
cycles: false
bibtex_author: Salim, Adil and Sun, Lukang and Richtarik, Peter
author:
- given: Adil
  family: Salim
- given: Lukang
  family: Sun
- given: Peter
  family: Richtarik
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/salim22a/salim22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
