---
title: Gradient Descent on Neurons and its Link to Approximate Second-order Optimization
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'Second-order optimizers are thought to hold the potential to speed up neural
  network training, but due to the enormous size of the curvature matrix, they typically
  require approximations to be computationally tractable. The most successful family
  of approximations are Kronecker-Factored, block-diagonal curvature estimates (KFAC).
  Here, we combine tools from prior work to evaluate exact second-order updates with
  careful ablations to establish a surprising result: Due to its approximations, KFAC
  is not closely related to second-order updates, and in particular, it significantly
  outperforms true second-order updates. This challenges widely held believes and
  immediately raises the question why KFAC performs so well. Towards answering this
  question we present evidence strongly suggesting that KFAC approximates a first-order
  algorithm, which performs gradient descent on neurons rather than weights. Finally,
  we show that this optimizer often improves over KFAC in terms of computational cost
  and data-efficiency.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: benzing22a
month: 0
tex_title: Gradient Descent on Neurons and its Link to Approximate Second-order Optimization
firstpage: 1817
lastpage: 1853
page: 1817-1853
order: 1817
cycles: false
bibtex_author: Benzing, Frederik
author:
- given: Frederik
  family: Benzing
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/benzing22a/benzing22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
