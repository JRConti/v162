---
title: Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: How to train deep neural networks (DNNs) to generalize well is a central
  concern in deep learning, especially for severely overparameterized networks nowadays.
  In this paper, we propose an effective method to improve the model generalization
  by additionally penalizing the gradient norm of loss function during optimization.
  We demonstrate that confining the gradient norm of loss function could help lead
  the optimizers towards finding flat minima. We leverage the first-order approximation
  to efficiently implement the corresponding gradient to fit well in the gradient
  descent framework. In our experiments, we confirm that when using our methods, generalization
  performance of various models could be improved on different datasets. Also, we
  show that the recent sharpness-aware minimization method (Foretet al., 2021) is
  a special, but not the best, case of our method, where the best case of our method
  could give new state-of-art performance on these tasks. Code is available at https://github.com/zhaoyang-0204/gnp.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao22i
month: 0
tex_title: Penalizing Gradient Norm for Efficiently Improving Generalization in Deep
  Learning
firstpage: 26982
lastpage: 26992
page: 26982-26992
order: 26982
cycles: false
bibtex_author: Zhao, Yang and Zhang, Hao and Hu, Xiuyuan
author:
- given: Yang
  family: Zhao
- given: Hao
  family: Zhang
- given: Xiuyuan
  family: Hu
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/zhao22i/zhao22i.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
