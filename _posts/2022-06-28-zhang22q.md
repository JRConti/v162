---
title: 'Neural Network Weights Do Not Converge to Stationary Points: An Invariant
  Measure Perspective'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: This work examines the deep disconnect between existing theoretical analyses
  of gradient-based algorithms and the practice of training deep neural networks.
  Specifically, we provide numerical evidence that in large-scale neural network training
  (e.g., ImageNet + ResNet101, and WT103 + TransformerXL models), the neural networkâ€™s
  weights do not converge to stationary points where the gradient of the loss is zero.
  Remarkably, however, we observe that even though the weights do not converge to
  stationary points, the progress in minimizing the loss function halts and training
  loss stabilizes. Inspired by this observation, we propose a new perspective based
  on ergodic theory of dynamical systems to explain it. Rather than studying the evolution
  of weights, we study the evolution of the distribution of weights. We prove convergence
  of the distribution of weights to an approximate invariant measure, thereby explaining
  how the training loss can stabilize without weights necessarily converging to stationary
  points. We further discuss how this perspective can better align optimization theory
  with empirical observations in machine learning practice.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang22q
month: 0
tex_title: 'Neural Network Weights Do Not Converge to Stationary Points: An Invariant
  Measure Perspective'
firstpage: 26330
lastpage: 26346
page: 26330-26346
order: 26330
cycles: false
bibtex_author: Zhang, Jingzhao and Li, Haochuan and Sra, Suvrit and Jadbabaie, Ali
author:
- given: Jingzhao
  family: Zhang
- given: Haochuan
  family: Li
- given: Suvrit
  family: Sra
- given: Ali
  family: Jadbabaie
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/zhang22q/zhang22q.pdf
extras:
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2022/supplementary/zhang22q-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
