---
title: Policy Gradient Method For Robust Reinforcement Learning
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: This paper develops the first policy gradient method with global optimality
  guarantee and complexity analysis for robust reinforcement learning under model
  mismatch. Robust reinforcement learning is to learn a policy robust to model mismatch
  between simulator and real environment. We first develop the robust policy (sub-)gradient,
  which is applicable for any differentiable parametric policy class. We show that
  the proposed robust policy gradient method converges to the global optimum asymptotically
  under direct policy parameterization. We further develop a smoothed robust policy
  gradient method, and show that to achieve an $\epsilon$-global optimum, the complexity
  is $\mathcal O(\epsilon^{-3})$. We then extend our methodology to the general model-free
  setting, and design the robust actor-critic method with differentiable parametric
  policy class and value function. We further characterize its asymptotic convergence
  and sample complexity under the tabular setting. Finally, we provide simulation
  results to demonstrate the robustness of our methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang22at
month: 0
tex_title: Policy Gradient Method For Robust Reinforcement Learning
firstpage: 23484
lastpage: 23526
page: 23484-23526
order: 23484
cycles: false
bibtex_author: Wang, Yue and Zou, Shaofeng
author:
- given: Yue
  family: Wang
- given: Shaofeng
  family: Zou
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/wang22at/wang22at.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
