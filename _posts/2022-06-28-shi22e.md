---
title: Visual Attention Emerges from Recurrent Sparse Reconstruction
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'Visual attention helps achieve robust perception under noise, corruption,
  and distribution shifts in human vision, which are areas where modern neural networks
  still fall short. We present VARS, Visual Attention from Recurrent Sparse reconstruction,
  a new attention formulation built on two prominent features of the human visual
  attention mechanism: recurrency and sparsity. Related features are grouped together
  via recurrent connections between neurons, with salient objects emerging via sparse
  regularization. VARS adopts an attractor network with recurrent connections that
  converges toward a stable pattern over time. Network layers are represented as ordinary
  differential equations (ODEs), formulating attention as a recurrent attractor network
  that equivalently optimizes the sparse reconstruction of input using a dictionary
  of “templates” encoding underlying patterns of data. We show that self-attention
  is a special case of VARS with a single-step optimization and no sparsity constraint.
  VARS can be readily used as a replacement for self-attention in popular vision transformers,
  consistently improving their robustness across various benchmarks.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shi22e
month: 0
tex_title: Visual Attention Emerges from Recurrent Sparse Reconstruction
firstpage: 20041
lastpage: 20056
page: 20041-20056
order: 20041
cycles: false
bibtex_author: Shi, Baifeng and Song, Yale and Joshi, Neel and Darrell, Trevor and
  Wang, Xin
author:
- given: Baifeng
  family: Shi
- given: Yale
  family: Song
- given: Neel
  family: Joshi
- given: Trevor
  family: Darrell
- given: Xin
  family: Wang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/shi22e/shi22e.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
