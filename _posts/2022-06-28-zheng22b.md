---
title: Linear Complexity Randomized Self-attention Mechanism
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Recently, random feature attentions (RFAs) are proposed to approximate the
  softmax attention in linear time and space complexity by linearizing the exponential
  kernel. In this paper, we first propose a novel perspective to understand the bias
  in such approximation by recasting RFAs as self-normalized importance samplers.
  This perspective further sheds light on an <em>unbiased</em> estimator for the whole
  softmax attention, called randomized attention (RA). RA constructs positive random
  features via query-specific distributions and enjoys greatly improved approximation
  fidelity, albeit exhibiting quadratic complexity. By combining the expressiveness
  in RA and the efficiency in RFA, we develop a novel linear complexity self-attention
  mechanism called linear randomized attention (LARA). Extensive experiments across
  various domains demonstrate that RA and LARA significantly improve the performance
  of RFAs by a substantial margin.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zheng22b
month: 0
tex_title: Linear Complexity Randomized Self-attention Mechanism
firstpage: 27011
lastpage: 27041
page: 27011-27041
order: 27011
cycles: false
bibtex_author: Zheng, Lin and Wang, Chong and Kong, Lingpeng
author:
- given: Lin
  family: Zheng
- given: Chong
  family: Wang
- given: Lingpeng
  family: Kong
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/zheng22b/zheng22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
