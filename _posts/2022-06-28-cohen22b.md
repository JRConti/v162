---
title: Diffusion bridges vector quantized variational autoencoders
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Vector Quantized-Variational AutoEncoders (VQ-VAE) are generative models
  based on discrete latent representations of the data, where inputs are mapped to
  a finite set of learned embeddings. To generate new samples, an autoregressive prior
  distribution over the discrete states must be trained separately. This prior is
  generally very complex and leads to slow generation. In this work, we propose a
  new model to train the prior and the encoder/decoder networks simultaneously. We
  build a diffusion bridge between a continuous coded vector and a non-informative
  prior distribution. The latent discrete states are then given as random functions
  of these continuous vectors. We show that our model is competitive with the autoregressive
  prior on the mini-Imagenet and CIFAR dataset and is efficient in both optimization
  and sampling. Our framework also extends the standard VQ-VAE and enables end-to-end
  training.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cohen22b
month: 0
tex_title: Diffusion bridges vector quantized variational autoencoders
firstpage: 4141
lastpage: 4156
page: 4141-4156
order: 4141
cycles: false
bibtex_author: Cohen, Max and Quispe, Guillaume and Corff, Sylvain Le and Ollion,
  Charles and Moulines, Eric
author:
- given: Max
  family: Cohen
- given: Guillaume
  family: Quispe
- given: Sylvain Le
  family: Corff
- given: Charles
  family: Ollion
- given: Eric
  family: Moulines
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/cohen22b/cohen22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
