---
title: Self-supervised Models are Good Teaching Assistants for Vision Transformers
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Transformers have shown remarkable progress on computer vision tasks in
  the past year. Compared to their CNN counterparts, transformers usually need the
  help of distillation to achieve comparable results on middle or small sized datasets.
  Meanwhile, recent researches discover that when transformers are trained with supervised
  and self-supervised manner respectively, the captured patterns are quite different
  both qualitatively and quantitatively. These findings motivate us to introduce an
  self-supervised teaching assistant (SSTA) besides the commonly used supervised teacher
  to improve the performance of transformers. Specifically, we propose a head-level
  knowledge distillation method that selects the most important head of the supervised
  teacher and self-supervised teaching assistant, and let the student mimic the attention
  distribution of these two heads, so as to make the student focus on the relationship
  between tokens deemed by the teacher and the teacher assistant. Extensive experiments
  verify the effectiveness of SSTA and demonstrate that the proposed SSTA is a good
  compensation to the supervised teacher. Meanwhile, some analytical experiments towards
  multiple perspectives (e.g. prediction, shape bias, robustness, and transferability
  to downstream tasks) with supervised teachers, self-supervised teaching assistants
  and students are inductive and may inspire future researches.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu22c
month: 0
tex_title: Self-supervised Models are Good Teaching Assistants for Vision Transformers
firstpage: 24031
lastpage: 24042
page: 24031-24042
order: 24031
cycles: false
bibtex_author: Wu, Haiyan and Gao, Yuting and Zhang, Yinqi and Lin, Shaohui and Xie,
  Yuan and Sun, Xing and Li, Ke
author:
- given: Haiyan
  family: Wu
- given: Yuting
  family: Gao
- given: Yinqi
  family: Zhang
- given: Shaohui
  family: Lin
- given: Yuan
  family: Xie
- given: Xing
  family: Sun
- given: Ke
  family: Li
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/wu22c/wu22c.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
