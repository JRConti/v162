---
title: Power-Law Escape Rate of SGD
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Stochastic gradient descent (SGD) undergoes complicated multiplicative noise
  for the mean-square loss. We use this property of SGD noise to derive a stochastic
  differential equation (SDE) with simpler additive noise by performing a random time
  change. Using this formalism, we show that the log loss barrier $\Delta\log L=\log[L(\theta^s)/L(\theta^*)]$
  between a local minimum $\theta^*$ and a saddle $\theta^s$ determines the escape
  rate of SGD from the local minimum, contrary to the previous results borrowing from
  physics that the linear loss barrier $\Delta L=L(\theta^s)-L(\theta^*)$ decides
  the escape rate. Our escape-rate formula strongly depends on the typical magnitude
  $h^*$ and the number $n$ of the outlier eigenvalues of the Hessian. This result
  explains an empirical fact that SGD prefers flat minima with low effective dimensions,
  giving an insight into implicit biases of SGD.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mori22a
month: 0
tex_title: Power-Law Escape Rate of {SGD}
firstpage: 15959
lastpage: 15975
page: 15959-15975
order: 15959
cycles: false
bibtex_author: Mori, Takashi and Ziyin, Liu and Liu, Kangqiao and Ueda, Masahito
author:
- given: Takashi
  family: Mori
- given: Liu
  family: Ziyin
- given: Kangqiao
  family: Liu
- given: Masahito
  family: Ueda
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/mori22a/mori22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
