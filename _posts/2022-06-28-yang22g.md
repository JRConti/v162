---
title: Efficient Variance Reduction for Meta-learning
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Meta-learning tries to learn meta-knowledge from a large number of tasks.
  However, the stochastic meta-gradient can have large variance due to data sampling
  (from each task) and task sampling (from the whole task distribution), leading to
  slow convergence. In this paper, we propose a novel approach that integrates variance
  reduction with first-order meta-learning algorithms such as Reptile. It retains
  the bilevel formulation which better captures the structure of meta-learning, but
  does not require storing the vast number of task-specific parameters in general
  bilevel variance reduction methods. Theoretical results show that it has fast convergence
  rate due to variance reduction. Experiments on benchmark few-shot classification
  data sets demonstrate its effectiveness over state-of-the-art meta-learning algorithms
  with and without variance reduction.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang22g
month: 0
tex_title: Efficient Variance Reduction for Meta-learning
firstpage: 25070
lastpage: 25095
page: 25070-25095
order: 25070
cycles: false
bibtex_author: Yang, Hansi and Kwok, James
author:
- given: Hansi
  family: Yang
- given: James
  family: Kwok
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/yang22g/yang22g.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
