---
title: 'NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Pretrained language models have become the standard approach for many NLP
  tasks due to strong performance, but they are very expensive to train. We propose
  a simple and efficient learning framework, TLM, that does not rely on large-scale
  pretraining. Given some labeled task data and a large general corpus, TLM uses task
  data as queries to retrieve a tiny subset of the general corpus and jointly optimizes
  the task objective and the language modeling objective from scratch. On eight classification
  datasets in four domains, TLM achieves results better than or similar to pretrained
  language models (e.g., RoBERTa-Large) while reducing the training FLOPs by two orders
  of magnitude. With high accuracy and efficiency, we hope TLM will contribute to
  democratizing NLP and expediting its development.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yao22c
month: 0
tex_title: "{NLP} From Scratch Without Large-Scale Pretraining: A Simple and Efficient
  Framework"
firstpage: 25438
lastpage: 25451
page: 25438-25451
order: 25438
cycles: false
bibtex_author: Yao, Xingcheng and Zheng, Yanan and Yang, Xiaocong and Yang, Zhilin
author:
- given: Xingcheng
  family: Yao
- given: Yanan
  family: Zheng
- given: Xiaocong
  family: Yang
- given: Zhilin
  family: Yang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/yao22c/yao22c.pdf
extras:
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2022/supplementary/yao22c-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
