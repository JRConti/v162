---
title: Nesterov Accelerated Shuffling Gradient Method for Convex Optimization
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: In this paper, we propose Nesterov Accelerated Shuffling Gradient (NASG),
  a new algorithm for the convex finite-sum minimization problems. Our method integrates
  the traditional Nesterovâ€™s acceleration momentum with different shuffling sampling
  schemes. We show that our algorithm has an improved rate of $\Ocal(1/T)$ using unified
  shuffling schemes, where $T$ is the number of epochs. This rate is better than that
  of any other shuffling gradient methods in convex regime. Our convergence analysis
  does not require an assumption on bounded domain or a bounded gradient condition.
  For randomized shuffling schemes, we improve the convergence bound further. When
  employing some initial condition, we show that our method converges faster near
  the small neighborhood of the solution. Numerical simulations demonstrate the efficiency
  of our algorithm.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tran22a
month: 0
tex_title: "{N}esterov Accelerated Shuffling Gradient Method for Convex Optimization"
firstpage: 21703
lastpage: 21732
page: 21703-21732
order: 21703
cycles: false
bibtex_author: Tran, Trang H and Scheinberg, Katya and Nguyen, Lam M
author:
- given: Trang H
  family: Tran
- given: Katya
  family: Scheinberg
- given: Lam M
  family: Nguyen
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/tran22a/tran22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
