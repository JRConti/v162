---
title: On the Convergence of Local Stochastic Compositional Gradient Descent with
  Momentum
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Federated Learning has been actively studied due to its efficiency in numerous
  real-world applications in the past few years. However, the federated stochastic
  compositional optimization problem is still underexplored, even though it has widespread
  applications in machine learning. In this paper, we developed a novel local stochastic
  compositional gradient descent with momentum method, which facilitates Federated
  Learning for the stochastic compositional problem. Importantly, we investigated
  the convergence rate of our proposed method and proved that it can achieve the $O(1/\epsilon^4)$
  sample complexity, which is better than existing methods. Meanwhile, our communication
  complexity $O(1/\epsilon^3)$ can match existing methods. To the best of our knowledge,
  this is the first work achieving such favorable sample and communication complexities.
  Additionally, extensive experimental results demonstrate the superior empirical
  performance over existing methods, confirming the efficacy of our method.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gao22c
month: 0
tex_title: On the Convergence of Local Stochastic Compositional Gradient Descent with
  Momentum
firstpage: 7017
lastpage: 7035
page: 7017-7035
order: 7017
cycles: false
bibtex_author: Gao, Hongchang and Li, Junyi and Huang, Heng
author:
- given: Hongchang
  family: Gao
- given: Junyi
  family: Li
- given: Heng
  family: Huang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/gao22c/gao22c.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
