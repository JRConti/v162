---
title: Rethinking Attention-Model Explainability through Faithfulness Violation Test
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'Attention mechanisms are dominating the explainability of deep models.
  They produce probability distributions over the input, which are widely deemed as
  feature-importance indicators. However, in this paper, we find one critical limitation
  in attention explanations: weakness in identifying the polarity of feature impact.
  This would be somehow misleading â€“ features with higher attention weights may not
  faithfully contribute to model predictions; instead, they can impose suppression
  effects. With this finding, we reflect on the explainability of current attention-based
  techniques, such as Attention $\bigodot$ Gradient and LRP-based attention explanations.
  We first propose an actionable diagnostic methodology (henceforth faithfulness violation
  test) to measure the consistency between explanation weights and the impact polarity.
  Through the extensive experiments, we then show that most tested explanation methods
  are unexpectedly hindered by the faithfulness violation issue, especially the raw
  attention. Empirical analyses on the factors affecting violation issues further
  provide useful observations for adopting explanation methods in attention models.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu22i
month: 0
tex_title: Rethinking Attention-Model Explainability through Faithfulness Violation
  Test
firstpage: 13807
lastpage: 13824
page: 13807-13824
order: 13807
cycles: false
bibtex_author: Liu, Yibing and Li, Haoliang and Guo, Yangyang and Kong, Chenqi and
  Li, Jing and Wang, Shiqi
author:
- given: Yibing
  family: Liu
- given: Haoliang
  family: Li
- given: Yangyang
  family: Guo
- given: Chenqi
  family: Kong
- given: Jing
  family: Li
- given: Shiqi
  family: Wang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/liu22i/liu22i.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
