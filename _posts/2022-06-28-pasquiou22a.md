---
title: Neural Language Models are not Born Equal to Fit Brain Data, but Training Helps
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Neural Language Models (NLMs) have made tremendous advances during the last
  years, achieving impressive performance on various linguistic tasks. Capitalizing
  on this, studies in neuroscience have started to use NLMs to study neural activity
  in the human brain during language processing. However, many questions remain unanswered
  regarding which factors determine the ability of a neural language model to capture
  brain activity (aka its ’brain score’). Here, we make first steps in this direction
  and examine the impact of test loss, training corpus and model architecture (comparing
  GloVe, LSTM, GPT-2 and BERT), on the prediction of functional Magnetic Resonance
  Imaging time-courses of participants listening to an audiobook. We find that (1)
  untrained versions of each model already explain significant amount of signal in
  the brain by capturing similarity in brain responses across identical words, with
  the untrained LSTM outperforming the transformer-based models, being less impacted
  by the effect of context; (2) that training NLP models improves brain scores in
  the same brain regions irrespective of the model’s architecture; (3) that Perplexity
  (test loss) is not a good predictor of brain score; (4) that training data have
  a strong influence on the outcome and, notably, that off-the-shelf models may lack
  statistical power to detect brain activations. Overall, we outline the impact of
  model-training choices, and suggest good practices for future studies aiming at
  explaining the human language system using neural language models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pasquiou22a
month: 0
tex_title: Neural Language Models are not Born Equal to Fit Brain Data, but Training
  Helps
firstpage: 17499
lastpage: 17516
page: 17499-17516
order: 17499
cycles: false
bibtex_author: Pasquiou, Alexandre and Lakretz, Yair and Hale, John T and Thirion,
  Bertrand and Pallier, Christophe
author:
- given: Alexandre
  family: Pasquiou
- given: Yair
  family: Lakretz
- given: John T
  family: Hale
- given: Bertrand
  family: Thirion
- given: Christophe
  family: Pallier
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/pasquiou22a/pasquiou22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
