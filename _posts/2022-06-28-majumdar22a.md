---
title: Feature selection using e-values
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: In the context of supervised learning, we introduce the concept of e-value.
  An e-value is a scalar quantity that represents the proximity of the sampling distribution
  of parameter estimates in a model trained on a subset of features to that of the
  model trained on all features (i.e. the full model). Under general conditions, a
  rank ordering of e-values separates models that contain all essential features from
  those that do not. For a p-dimensional feature space, this requires fitting only
  the full model and evaluating p+1 models, as opposed to the traditional requirement
  of fitting and evaluating 2^p models. The above e-values framework is applicable
  to a wide range of parametric models. We use data depths and a fast resampling-based
  algorithm to implement a feature selection procedure, providing consistency results.
  Through experiments across several model settings and synthetic and real datasets,
  we establish that the e-values can be a promising general alternative to existing
  model-specific methods of feature selection.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: majumdar22a
month: 0
tex_title: Feature selection using e-values
firstpage: 14753
lastpage: 14773
page: 14753-14773
order: 14753
cycles: false
bibtex_author: Majumdar, Subhabrata and Chatterjee, Snigdhansu
author:
- given: Subhabrata
  family: Majumdar
- given: Snigdhansu
  family: Chatterjee
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/majumdar22a/majumdar22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
