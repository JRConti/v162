---
title: What Language Model Architecture and Pretraining Objective Works Best for Zero-Shot
  Generalization?
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Large pretrained Transformer language models have been shown to exhibit
  zero-shot generalization, i.e. they can perform a wide variety of tasks that they
  were not explicitly trained on. However, the architectures and pretraining objectives
  used across state-of-the-art models differ significantly, and there has been limited
  systematic comparison of these factors. In this work, we present a large-scale evaluation
  of modeling choices and their impact on zero-shot generalization. In particular,
  we focus on text-to-text models and experiment with three model architectures (causal/non-causal
  decoder-only and encoder-decoder), trained with two different pretraining objectives
  (autoregressive and masked language modeling), and evaluated with and without multitask
  prompted finetuning. We train models with over 5 billion parameters for more than
  168 billion tokens, thereby increasing the likelihood that our conclusions will
  transfer to even larger scales. Our experiments show that causal decoder-only models
  trained on an autoregressive language modeling objective exhibit the strongest zero-shot
  generalization after purely self-supervised pretraining. However, models with non-causal
  visibility on their input trained with a masked language modeling objective followed
  by multitask finetuning perform the best among our experiments. We therefore consider
  the adaptation of pretrained models across architectures and objectives. Code and
  checkpoints are available at https://github.com/bigscience- workshop/architecture-objective.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang22u
month: 0
tex_title: What Language Model Architecture and Pretraining Objective Works Best for
  Zero-Shot Generalization?
firstpage: 22964
lastpage: 22984
page: 22964-22984
order: 22964
cycles: false
bibtex_author: Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Scao, Teven
  Le and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin
author:
- given: Thomas
  family: Wang
- given: Adam
  family: Roberts
- given: Daniel
  family: Hesslow
- given: Teven Le
  family: Scao
- given: Hyung Won
  family: Chung
- given: Iz
  family: Beltagy
- given: Julien
  family: Launay
- given: Colin
  family: Raffel
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/wang22u/wang22u.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
