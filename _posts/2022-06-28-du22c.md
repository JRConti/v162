---
title: 'GLaM: Efficient Scaling of Language Models with Mixture-of-Experts'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Scaling language models with more data, compute and parameters has driven
  significant progress in natural language processing. For example, thanks to scaling,
  GPT-3 was able to achieve strong results on in-context learning tasks. However,
  training these large dense models requires significant amounts of computing resources.
  In this paper, we propose and develop a family of language models named \glam (\textbf{G}eneralist
  \textbf{La}nguage \textbf{M}odel), which uses a sparsely activated mixture-of-experts
  architecture to scale the model capacity while also incurring substantially less
  training cost compared to dense variants. The largest \glam has 1.2 trillion parameters,
  which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy
  used to train GPT-3 and requires half of the computation flops for inference, while
  still achieving better overall fewshot performance across 29 NLP tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: du22c
month: 0
tex_title: "{GL}a{M}: Efficient Scaling of Language Models with Mixture-of-Experts"
firstpage: 5547
lastpage: 5569
page: 5547-5569
order: 5547
cycles: false
bibtex_author: Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin,
  Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and
  Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten P and Zhou, Zongwei
  and Wang, Tao and Wang, Emma and Webster, Kellie and Pellat, Marie and Robinson,
  Kevin and Meier-Hellstern, Kathleen and Duke, Toju and Dixon, Lucas and Zhang, Kun
  and Le, Quoc and Wu, Yonghui and Chen, Zhifeng and Cui, Claire
author:
- given: Nan
  family: Du
- given: Yanping
  family: Huang
- given: Andrew M
  family: Dai
- given: Simon
  family: Tong
- given: Dmitry
  family: Lepikhin
- given: Yuanzhong
  family: Xu
- given: Maxim
  family: Krikun
- given: Yanqi
  family: Zhou
- given: Adams Wei
  family: Yu
- given: Orhan
  family: Firat
- given: Barret
  family: Zoph
- given: Liam
  family: Fedus
- given: Maarten P
  family: Bosma
- given: Zongwei
  family: Zhou
- given: Tao
  family: Wang
- given: Emma
  family: Wang
- given: Kellie
  family: Webster
- given: Marie
  family: Pellat
- given: Kevin
  family: Robinson
- given: Kathleen
  family: Meier-Hellstern
- given: Toju
  family: Duke
- given: Lucas
  family: Dixon
- given: Kun
  family: Zhang
- given: Quoc
  family: Le
- given: Yonghui
  family: Wu
- given: Zhifeng
  family: Chen
- given: Claire
  family: Cui
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/du22c/du22c.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
