---
title: 'QSFL: A Two-Level Uplink Communication Optimization Framework for Federated
  Learning'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: In cross-device Federated Learning (FL), the communication cost of transmitting
  full-precision models between edge devices and a central server is a significant
  bottleneck, due to expensive, unreliable, and low-bandwidth wireless connections.
  As a solution, we propose a novel FL framework named QSFL, towards optimizing FL
  uplink (client-to-server) communication at both client and model levels. At the
  client level, we design a Qualification Judgment (QJ) algorithm to sample high-qualification
  clients to upload models. At the model level, we explore a Sparse Cyclic Sliding
  Segment (SCSS) algorithm to further compress transmitted models. We prove that QSFL
  can converge over wall-to-wall time, and develop an optimal hyperparameter searching
  algorithm based on theoretical analysis to enable QSFL to make the best trade-off
  between model accuracy and communication cost. Experimental results show that QSFL
  achieves state-of-the-art compression ratios with marginal model accuracy degradation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yi22a
month: 0
tex_title: "{QSFL}: A Two-Level Uplink Communication Optimization Framework for Federated
  Learning"
firstpage: 25501
lastpage: 25513
page: 25501-25513
order: 25501
cycles: false
bibtex_author: Yi, Liping and Gang, Wang and Xiaoguang, Liu
author:
- given: Liping
  family: Yi
- given: Wang
  family: Gang
- given: Liu
  family: Xiaoguang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/yi22a/yi22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
