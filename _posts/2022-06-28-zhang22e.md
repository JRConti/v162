---
title: 'Stochastic Continuous Submodular Maximization: Boosting via Non-oblivious
  Function'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: In this paper, we revisit Stochastic Continuous Submodular Maximization
  in both offline and online settings, which can benefit wide applications in machine
  learning and operations research areas. We present a boosting framework covering
  gradient ascent and online gradient ascent. The fundamental ingredient of our methods
  is a novel non-oblivious function $F$ derived from a factor-revealing optimization
  problem, whose any stationary point provides a $(1-e^{-\gamma})$-approximation to
  the global maximum of the $\gamma$-weakly DR-submodular objective function $f\in
  C^{1,1}_L(\mathcal{X})$. Under the offline scenario, we propose a boosting gradient
  ascent method achieving $(1-e^{-\gamma}-\epsilon^{2})$-approximation after $O(1/\epsilon^2)$
  iterations, which improves the $(\frac{\gamma^2}{1+\gamma^2})$ approximation ratio
  of the classical gradient ascent algorithm. In the online setting, for the first
  time we consider the adversarial delays for stochastic gradient feedback, under
  which we propose a boosting online gradient algorithm with the same non-oblivious
  function $F$. Meanwhile, we verify that this boosting online algorithm achieves
  a regret of $O(\sqrt{D})$ against a $(1-e^{-\gamma})$-approximation to the best
  feasible solution in hindsight, where $D$ is the sum of delays of gradient feedback.
  To the best of our knowledge, this is the first result to obtain $O(\sqrt{T})$ regret
  against a $(1-e^{-\gamma})$-approximation with $O(1)$ gradient inquiry at each time
  step, when no delay exists, i.e., $D=T$. Finally, numerical experiments demonstrate
  the effectiveness of our boosting methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang22e
month: 0
tex_title: 'Stochastic Continuous Submodular Maximization: Boosting via Non-oblivious
  Function'
firstpage: 26116
lastpage: 26134
page: 26116-26134
order: 26116
cycles: false
bibtex_author: Zhang, Qixin and Deng, Zengde and Chen, Zaiyi and Hu, Haoyuan and Yang,
  Yu
author:
- given: Qixin
  family: Zhang
- given: Zengde
  family: Deng
- given: Zaiyi
  family: Chen
- given: Haoyuan
  family: Hu
- given: Yu
  family: Yang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/zhang22e/zhang22e.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
