---
title: Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Accurate probabilistic predictions can be characterized by two properties{—}calibration
  and sharpness. However, standard maximum likelihood training yields models that
  are poorly calibrated and thus inaccurate{—}a 90% confidence interval typically
  does not contain the true outcome 90% of the time. This paper argues that calibration
  is important in practice and is easy to maintain by performing low-dimensional density
  estimation. We introduce a simple training procedure based on recalibration that
  yields calibrated models without sacrificing overall performance; unlike previous
  approaches, ours ensures the most general property of distribution calibration and
  applies to any model, including neural networks. We formally prove the correctness
  of our procedure assuming that we can estimate densities in low dimensions and we
  establish uniform convergence bounds. Our results yield empirical performance improvements
  on linear and deep Bayesian models and suggest that calibration should be increasingly
  leveraged across machine learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kuleshov22a
month: 0
tex_title: Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation
firstpage: 11683
lastpage: 11693
page: 11683-11693
order: 11683
cycles: false
bibtex_author: Kuleshov, Volodymyr and Deshpande, Shachi
author:
- given: Volodymyr
  family: Kuleshov
- given: Shachi
  family: Deshpande
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/kuleshov22a/kuleshov22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
