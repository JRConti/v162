---
title: 'Constants Matter: The Performance Gains of Active Learning'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Within machine learning, active learning studies the gains in performance
  made possible by adaptively selecting data points to label. In this work, we show
  through upper and lower bounds, that for a simple benign setting of well-specified
  logistic regression on a uniform distribution over a sphere, the expected excess
  error of both active learning and random sampling have the same inverse proportional
  dependence on the number of samples. Importantly, due to the nature of lower bounds,
  any more general setting does not allow a better dependence on the number of samples.
  Additionally, we show a variant of uncertainty sampling can achieve a faster rate
  of convergence than random sampling by a factor of the Bayes error, a recent empirical
  observation made by other work. Qualitatively, this work is pessimistic with respect
  to the asymptotic dependence on the number of samples, but optimistic with respect
  to finding performance gains in the constants.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mussmann22a
month: 0
tex_title: 'Constants Matter: The Performance Gains of Active Learning'
firstpage: 16123
lastpage: 16173
page: 16123-16173
order: 16123
cycles: false
bibtex_author: Mussmann, Stephen O and Dasgupta, Sanjoy
author:
- given: Stephen O
  family: Mussmann
- given: Sanjoy
  family: Dasgupta
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/mussmann22a/mussmann22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
