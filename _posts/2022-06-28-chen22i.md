---
title: Learning Infinite-horizon Average-reward Markov Decision Process with Constraints
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: We study regret minimization for infinite-horizon average-reward Markov
  Decision Processes (MDPs) under cost constraints. We start by designing a policy
  optimization algorithm with carefully designed action-value estimator and bonus
  term, and show that for ergodic MDPs, our algorithm ensures $O(\sqrt{T})$ regret
  and constant constraint violation, where $T$ is the total number of time steps.
  This strictly improves over the algorithm of (Singh et al., 2020), whose regret
  and constraint violation are both $O(T^{2/3})$. Next, we consider the most general
  class of weakly communicating MDPs. Through a finite-horizon approximation, we develop
  another algorithm with $O(T^{2/3})$ regret and constraint violation, which can be
  further improved to $O(\sqrt{T})$ via a simple modification, albeit making the algorithm
  computationally inefficient. As far as we know, these are the first set of provable
  algorithms for weakly communicating MDPs with cost constraints.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen22i
month: 0
tex_title: Learning Infinite-horizon Average-reward {M}arkov Decision Process with
  Constraints
firstpage: 3246
lastpage: 3270
page: 3246-3270
order: 3246
cycles: false
bibtex_author: Chen, Liyu and Jain, Rahul and Luo, Haipeng
author:
- given: Liyu
  family: Chen
- given: Rahul
  family: Jain
- given: Haipeng
  family: Luo
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/chen22i/chen22i.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
