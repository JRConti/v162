---
title: "(Non-)Convergence Results for Predictive Coding Networks"
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Predictive coding networks (PCNs) are (un)supervised learning models, coming
  from neuroscience, that approximate how the brain works. One major open problem
  around PCNs is their convergence behavior. In this paper, we use dynamical systems
  theory to formally investigate the convergence of PCNs as they are used in machine
  learning. Doing so, we put their theory on a firm, rigorous basis, by developing
  a precise mathematical framework for PCN and show that for sufficiently small weights
  and initializations, PCNs converge for any input. Thereby, we provide the theoretical
  assurance that previous implementations, whose convergence was assessed solely by
  numerical experiments, can indeed capture the correct behavior of PCNs. Outside
  of the identified regime of small weights and small initializations, we show via
  a counterexample that PCNs can diverge, countering common beliefs held in the community.
  This is achieved by identifying a Neimark-Sacker bifurcation in a PCN of small size,
  which gives rise to an unstable fixed point and an invariant curve around it.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: frieder22a
month: 0
tex_title: "({N}on-){C}onvergence Results for Predictive Coding Networks"
firstpage: 6793
lastpage: 6810
page: 6793-6810
order: 6793
cycles: false
bibtex_author: Frieder, Simon and Lukasiewicz, Thomas
author:
- given: Simon
  family: Frieder
- given: Thomas
  family: Lukasiewicz
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/frieder22a/frieder22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
