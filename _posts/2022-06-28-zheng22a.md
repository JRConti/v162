---
title: Ripple Attention for Visual Perception with Sub-quadratic Complexity
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Transformer architectures are now central to sequence modeling tasks. At
  its heart is the attention mechanism, which enables effective modeling of long-term
  dependencies in a sequence. Recently, transformers have been successfully applied
  in the computer vision domain, where 2D images are first segmented into patches
  and then treated as 1D sequences. Such linearization, however, impairs the notion
  of spatial locality in images, which bears important visual clues. To bridge the
  gap, we propose <em>ripple attention</em>, a sub-quadratic attention mechanism for
  vision transformers. Built upon the recent kernel-based efficient attention mechanisms,
  we design a novel dynamic programming algorithm that weights contributions of different
  tokens to a query with respect to their relative spatial distances in the 2D space
  in linear observed time. Extensive experiments and analyses demonstrate the effectiveness
  of ripple attention on various visual tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zheng22a
month: 0
tex_title: Ripple Attention for Visual Perception with Sub-quadratic Complexity
firstpage: 26993
lastpage: 27010
page: 26993-27010
order: 26993
cycles: false
bibtex_author: Zheng, Lin and Pan, Huijie and Kong, Lingpeng
author:
- given: Lin
  family: Zheng
- given: Huijie
  family: Pan
- given: Lingpeng
  family: Kong
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/zheng22a/zheng22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
