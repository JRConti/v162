---
title: Off-Policy Evaluation for Large Action Spaces via Embeddings
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Off-policy evaluation (OPE) in contextual bandits has seen rapid adoption
  in real-world systems, since it enables offline evaluation of new policies using
  only historic log data. Unfortunately, when the number of actions is large, existing
  OPE estimators – most of which are based on inverse propensity score weighting –
  degrade severely and can suffer from extreme bias and variance. This foils the use
  of OPE in many applications from recommender systems to language models. To overcome
  this issue, we propose a new OPE estimator that leverages marginalized importance
  weights when action embeddings provide structure in the action space. We characterize
  the bias, variance, and mean squared error of the proposed estimator and analyze
  the conditions under which the action embedding provides statistical benefits over
  conventional estimators. In addition to the theoretical analysis, we find that the
  empirical performance improvement can be substantial, enabling reliable OPE even
  when existing estimators collapse due to a large number of actions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: saito22a
month: 0
tex_title: Off-Policy Evaluation for Large Action Spaces via Embeddings
firstpage: 19089
lastpage: 19122
page: 19089-19122
order: 19089
cycles: false
bibtex_author: Saito, Yuta and Joachims, Thorsten
author:
- given: Yuta
  family: Saito
- given: Thorsten
  family: Joachims
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/saito22a/saito22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
