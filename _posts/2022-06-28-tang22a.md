---
title: Biased Gradient Estimate with Drastic Variance Reduction for Meta Reinforcement
  Learning
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Despite the empirical success of meta reinforcement learning (meta-RL),
  there are still a number poorly-understood discrepancies between theory and practice.
  Critically, biased gradient estimates are almost always implemented in practice,
  whereas prior theory on meta-RL only establishes convergence under unbiased gradient
  estimates. In this work, we investigate such a discrepancy. In particular, (1) We
  show that unbiased gradient estimates have variance $\Theta(N)$ which linearly depends
  on the sample size $N$ of the inner loop updates; (2) We propose linearized score
  function (LSF) gradient estimates, which have bias $\mathcal{O}(1/\sqrt{N})$ and
  variance $\mathcal{O}(1/N)$; (3) We show that most empirical prior work in fact
  implements variants of the LSF gradient estimates. This implies that practical algorithms
  "accidentally" introduce bias to achieve better performance; (4) We establish theoretical
  guarantees for the LSF gradient estimates in meta-RL regarding its convergence to
  stationary points, showing better dependency on $N$ than prior work when $N$ is
  large.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tang22a
month: 0
tex_title: Biased Gradient Estimate with Drastic Variance Reduction for Meta Reinforcement
  Learning
firstpage: 21050
lastpage: 21075
page: 21050-21075
order: 21050
cycles: false
bibtex_author: Tang, Yunhao
author:
- given: Yunhao
  family: Tang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/tang22a/tang22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
