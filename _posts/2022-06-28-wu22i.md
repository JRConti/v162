---
title: 'Understanding Policy Gradient Algorithms: A Sensitivity-Based Approach'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: The REINFORCE algorithm \cite{williams1992simple} is popular in policy gradient
  (PG) for solving reinforcement learning (RL) problems. Meanwhile, the theoretical
  form of PG is fromÂ \cite{sutton1999policy}. Although both formulae prescribe PG,
  their precise connections are not yet illustrated. Recently, \citeauthor{nota2020policy}
  (\citeyear{nota2020policy}) have found that the ambiguity causes implementation
  errors. Motivated by the ambiguity and implementation incorrectness, we study PG
  from a perturbation perspective. In particular, we derive PG in a unified framework,
  precisely clarify the relation between PG implementation and theory, and echos back
  the findings by \citeauthor{nota2020policy}. Diving into factors contributing to
  empirical successes of the existing erroneous implementations, we find that small
  approximation error and the experience replay mechanism play critical roles.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu22i
month: 0
tex_title: 'Understanding Policy Gradient Algorithms: A Sensitivity-Based Approach'
firstpage: 24131
lastpage: 24149
page: 24131-24149
order: 24131
cycles: false
bibtex_author: Wu, Shuang and Shi, Ling and Wang, Jun and Tian, Guangjian
author:
- given: Shuang
  family: Wu
- given: Ling
  family: Shi
- given: Jun
  family: Wang
- given: Guangjian
  family: Tian
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/wu22i/wu22i.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
