---
title: Understanding the unstable convergence of gradient descent
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Most existing analyses of (stochastic) gradient descent rely on the condition
  that for $L$-smooth costs, the step size is less than $2/L$. However, many works
  have observed that in machine learning applications step sizes often do not fulfill
  this condition, yet (stochastic) gradient descent still converges, albeit in an
  unstable manner. We investigate this unstable convergence phenomenon from first
  principles, and discuss key causes behind it. We also identify its main characteristics,
  and how they interrelate based on both theory and experiments, offering a principled
  view toward understanding the phenomenon.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ahn22a
month: 0
tex_title: Understanding the unstable convergence of gradient descent
firstpage: 247
lastpage: 257
page: 247-257
order: 247
cycles: false
bibtex_author: Ahn, Kwangjun and Zhang, Jingzhao and Sra, Suvrit
author:
- given: Kwangjun
  family: Ahn
- given: Jingzhao
  family: Zhang
- given: Suvrit
  family: Sra
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/ahn22a/ahn22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
