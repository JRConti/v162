---
title: Learning Multiscale Transformer Models for Sequence Generation
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Multiscale feature hierarchies have been witnessed the success in the computer
  vision area. This further motivates researchers to design multiscale Transformer
  for natural language processing, mostly based on the self-attention mechanism. For
  example, restricting the receptive field across heads or extracting local fine-grained
  features via convolutions. However, most of existing works directly modeled local
  features but ignored the word-boundary information. This results in redundant and
  ambiguous attention distributions, which lacks of interpretability. In this work,
  we define those scales in different linguistic units, including sub-words, words
  and phrases. We built a multiscale Transformer model by establishing relationships
  among scales based on word-boundary information and phrase-level prior knowledge.
  The proposed \textbf{U}niversal \textbf{M}ulti\textbf{S}cale \textbf{T}ransformer,
  namely \textsc{Umst}, was evaluated on two sequence generation tasks. Notably, it
  yielded consistent performance gains over the strong baseline on several test sets
  without sacrificing the efficiency.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li22ac
month: 0
tex_title: Learning Multiscale Transformer Models for Sequence Generation
firstpage: 13225
lastpage: 13241
page: 13225-13241
order: 13225
cycles: false
bibtex_author: Li, Bei and Zheng, Tong and Jing, Yi and Jiao, Chengbo and Xiao, Tong
  and Zhu, Jingbo
author:
- given: Bei
  family: Li
- given: Tong
  family: Zheng
- given: Yi
  family: Jing
- given: Chengbo
  family: Jiao
- given: Tong
  family: Xiao
- given: Jingbo
  family: Zhu
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/li22ac/li22ac.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
