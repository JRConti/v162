---
title: Inductive Biases and Variable Creation in Self-Attention Mechanisms
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'Self-attention, an architectural motif designed to model long-range interactions
  in sequential data, has driven numerous recent breakthroughs in natural language
  processing and beyond. This work provides a theoretical analysis of the inductive
  biases of self-attention modules. Our focus is to rigorously establish which functions
  and long-range dependencies self-attention blocks prefer to represent. Our main
  result shows that bounded-norm Transformer networks "create sparse variables": a
  single self-attention head can represent a sparse function of the input sequence,
  with sample complexity scaling only logarithmically with the context length. To
  support our analysis, we present synthetic experiments to probe the sample complexity
  of learning sparse Boolean functions with Transformers.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: edelman22a
month: 0
tex_title: Inductive Biases and Variable Creation in Self-Attention Mechanisms
firstpage: 5793
lastpage: 5831
page: 5793-5831
order: 5793
cycles: false
bibtex_author: Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril
author:
- given: Benjamin L
  family: Edelman
- given: Surbhi
  family: Goel
- given: Sham
  family: Kakade
- given: Cyril
  family: Zhang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/edelman22a/edelman22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
