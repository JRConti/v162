---
title: Branching Reinforcement Learning
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: In this paper, we propose a novel Branching Reinforcement Learning (Branching
  RL) model, and investigate both Regret Minimization (RM) and Reward-Free Exploration
  (RFE) metrics for this model. Unlike standard RL where the trajectory of each episode
  is a single $H$-step path, branching RL allows an agent to take multiple base actions
  in a state such that transitions branch out to multiple successor states correspondingly,
  and thus it generates a tree-structured trajectory. This model finds important applications
  in hierarchical recommendation systems and online advertising. For branching RL,
  we establish new Bellman equations and key lemmas, i.e., branching value difference
  lemma and branching law of total variance, and also bound the total variance by
  only $O(H^2)$ under an exponentially-large trajectory. For RM and RFE metrics, we
  propose computationally efficient algorithms BranchVI and BranchRFE, respectively,
  and derive nearly matching upper and lower bounds. Our regret and sample complexity
  results are polynomial in all problem parameters despite exponentially-large trajectories.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: du22a
month: 0
tex_title: Branching Reinforcement Learning
firstpage: 5494
lastpage: 5530
page: 5494-5530
order: 5494
cycles: false
bibtex_author: Du, Yihan and Chen, Wei
author:
- given: Yihan
  family: Du
- given: Wei
  family: Chen
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/du22a/du22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
