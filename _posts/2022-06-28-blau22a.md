---
title: Optimizing Sequential Experimental Design with Deep Reinforcement Learning
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Bayesian approaches developed to solve the optimal design of sequential
  experiments are mathematically elegant but computationally challenging. Recently,
  techniques using amortization have been proposed to make these Bayesian approaches
  practical, by training a parameterized policy that proposes designs efficiently
  at deployment time. However, these methods may not sufficiently explore the design
  space, require access to a differentiable probabilistic model and can only optimize
  over continuous design spaces. Here, we address these limitations by showing that
  the problem of optimizing policies can be reduced to solving a Markov decision process
  (MDP). We solve the equivalent MDP with modern deep reinforcement learning techniques.
  Our experiments show that our approach is also computationally efficient at deployment
  time and exhibits state-of-the-art performance on both continuous and discrete design
  spaces, even when the probabilistic model is a black box.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: blau22a
month: 0
tex_title: Optimizing Sequential Experimental Design with Deep Reinforcement Learning
firstpage: 2107
lastpage: 2128
page: 2107-2128
order: 2107
cycles: false
bibtex_author: Blau, Tom and Bonilla, Edwin V. and Chades, Iadine and Dezfouli, Amir
author:
- given: Tom
  family: Blau
- given: Edwin V.
  family: Bonilla
- given: Iadine
  family: Chades
- given: Amir
  family: Dezfouli
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/blau22a/blau22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
