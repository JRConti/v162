---
title: Analyzing and Mitigating Interference in Neural Architecture Search
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'Weight sharing is a popular approach to reduce the training cost of neural
  architecture search (NAS) by reusing the weights of shared operators from previously
  trained child models. However, the rank correlation between the estimated accuracy
  and ground truth accuracy of those child models is low due to the interference among
  different child models caused by weight sharing. In this paper, we investigate the
  interference issue by sampling different child models and calculating the gradient
  similarity of shared operators, and observe that: 1) the interference on a shared
  operator between two child models is positively correlated with the number of different
  operators between them; 2) the interference is smaller when the inputs and outputs
  of the shared operator are more similar. Inspired by these two observations, we
  propose two approaches to mitigate the interference: 1) rather than randomly sampling
  child models for optimization, we propose a gradual modification scheme by modifying
  one operator between adjacent optimization steps to minimize the interference on
  the shared operators; 2) forcing the inputs and outputs of the operator across all
  child models to be similar to reduce the interference. Experiments on a BERT search
  space verify that mitigating interference via each of our proposed methods improves
  the rank correlation of super-pet and combining both methods can achieve better
  results. Our discovered architecture outperforms RoBERTa$_{\rm base}$ by 1.1 and
  0.6 points and ELECTRA$_{\rm base}$ by 1.6 and 1.1 points on the dev and test set
  of GLUE benchmark. Extensive results on the BERT compression, reading comprehension
  and large-scale image classification tasks also demonstrate the effectiveness and
  generality of our proposed methods.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu22h
month: 0
tex_title: Analyzing and Mitigating Interference in Neural Architecture Search
firstpage: 24646
lastpage: 24662
page: 24646-24662
order: 24646
cycles: false
bibtex_author: Xu, Jin and Tan, Xu and Song, Kaitao and Luo, Renqian and Leng, Yichong
  and Qin, Tao and Liu, Tie-Yan and Li, Jian
author:
- given: Jin
  family: Xu
- given: Xu
  family: Tan
- given: Kaitao
  family: Song
- given: Renqian
  family: Luo
- given: Yichong
  family: Leng
- given: Tao
  family: Qin
- given: Tie-Yan
  family: Liu
- given: Jian
  family: Li
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/xu22h/xu22h.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
