---
title: 'Data Scaling Laws in NMT: The Effect of Noise and Architecture'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'In this work, we study the effect of varying the architecture and training
  data quality on the data scaling properties of Neural Machine Translation (NMT).
  First, we establish that the test loss of encoder-decoder transformer models scales
  as a power law in the number of training samples, with a dependence on the model
  size. Then, we systematically vary aspects of the training setup to understand how
  they impact the data scaling laws. In particular, we change the following (1) Architecture
  and task setup: We compare to a transformer-LSTM hybrid, and a decoder-only transformer
  with a language modeling loss (2) Noise level in the training distribution: We experiment
  with filtering, and adding iid synthetic noise. In all the above cases, we find
  that the data scaling exponents are minimally impacted, suggesting that marginally
  worse architectures or training data can be compensated for by adding more data.
  Lastly, we find that using back-translated data instead of parallel data, can significantly
  degrade the scaling exponent.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bansal22b
month: 0
tex_title: 'Data Scaling Laws in {NMT}: The Effect of Noise and Architecture'
firstpage: 1466
lastpage: 1482
page: 1466-1482
order: 1466
cycles: false
bibtex_author: Bansal, Yamini and Ghorbani, Behrooz and Garg, Ankush and Zhang, Biao
  and Cherry, Colin and Neyshabur, Behnam and Firat, Orhan
author:
- given: Yamini
  family: Bansal
- given: Behrooz
  family: Ghorbani
- given: Ankush
  family: Garg
- given: Biao
  family: Zhang
- given: Colin
  family: Cherry
- given: Behnam
  family: Neyshabur
- given: Orhan
  family: Firat
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/bansal22b/bansal22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
