---
title: Transformers are Meta-Reinforcement Learners
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: The transformer architecture and variants presented a remarkable success
  across many machine learning tasks in recent years. This success is intrinsically
  related to the capability of handling long sequences and the presence of context-dependent
  weights from the attention mechanism. We argue that these capabilities suit the
  central role of a Meta-Reinforcement Learning algorithm. Indeed, a meta-RL agent
  needs to infer the task from a sequence of trajectories. Furthermore, it requires
  a fast adaptation strategy to adapt its policy for a new task - which can be achieved
  using the self-attention mechanism. In this work, we present TrMRL (Transformers
  for Meta-Reinforcement Learning), a meta-RL agent that mimics the memory reinstatement
  mechanism using the transformer architecture. It associates the recent past of working
  memories to build an episodic memory recursively through the transformer layers.
  We show that the self-attention computes a consensus representation that minimizes
  the Bayes Risk at each layer and provides meaningful features to compute the best
  actions. We conducted experiments in high-dimensional continuous control environments
  for locomotion and dexterous manipulation. Results show that TrMRL presents comparable
  or superior asymptotic performance, sample efficiency, and out-of-distribution generalization
  compared to the baselines in these environments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: melo22a
month: 0
tex_title: Transformers are Meta-Reinforcement Learners
firstpage: 15340
lastpage: 15359
page: 15340-15359
order: 15340
cycles: false
bibtex_author: Melo, Luckeciano C
author:
- given: Luckeciano C
  family: Melo
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/melo22a/melo22a.pdf
extras:
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2022/supplementary/melo22a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
