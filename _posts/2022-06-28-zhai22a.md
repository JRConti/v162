---
title: Position Prediction as an Effective Pretraining Strategy
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Transformers \cite{transformer} have gained increasing popularity in a wide
  range of applications, including Natural Language Processing (NLP), Computer Vision
  and Speech Recognition, because of their powerful representational capacity. However,
  harnessing this representational capacity effectively requires a large amount of
  data, strong regularization, or both, to mitigate overfitting. Recently, the power
  of the Transformer has been unlocked by self-supervised pretraining strategies based
  on masked autoencoders which rely on reconstructing masked inputs, directly, or
  contrastively from unmasked content. This pretraining strategy which has been used
  in BERT models in NLP \cite{bert}, Wav2Vec models in Speech \cite{wv2v2} and, recently,
  in MAE models in Vision \cite{beit, mae}, forces the model to learn about relationships
  between the content in different parts of the input using autoencoding related objectives.
  In this paper, we propose a novel, but surprisingly simple alternative to content
  reconstruction – that of predicting locations from content, without providing positional
  information for it. Doing so requires the Transformer to understand the positional
  relationships between different parts of the input, from their content alone. This
  amounts to an efficient implementation where the pretext task is a classification
  problem among all possible positions for each input token. We experiment on both
  Vision and Speech benchmarks, where our approach brings improvements over strong
  supervised training baselines and is comparable to modern unsupervised/self-supervised
  pretraining methods. Our method also enables Transformers trained without position
  embeddings to outperform ones trained with full position information.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhai22a
month: 0
tex_title: Position Prediction as an Effective Pretraining Strategy
firstpage: 26010
lastpage: 26027
page: 26010-26027
order: 26010
cycles: false
bibtex_author: Zhai, Shuangfei and Jaitly, Navdeep and Ramapuram, Jason and Busbridge,
  Dan and Likhomanenko, Tatiana and Cheng, Joseph Y and Talbott, Walter and Huang,
  Chen and Goh, Hanlin and Susskind, Joshua M
author:
- given: Shuangfei
  family: Zhai
- given: Navdeep
  family: Jaitly
- given: Jason
  family: Ramapuram
- given: Dan
  family: Busbridge
- given: Tatiana
  family: Likhomanenko
- given: Joseph Y
  family: Cheng
- given: Walter
  family: Talbott
- given: Chen
  family: Huang
- given: Hanlin
  family: Goh
- given: Joshua M
  family: Susskind
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/zhai22a/zhai22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
