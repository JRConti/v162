---
title: Robust Training of Neural Networks Using Scale Invariant Architectures
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'In contrast to SGD, adaptive gradient methods like Adam allow robust training
  of modern deep networks, especially large language models. However, the use of adaptivity
  not only comes at the cost of extra memory but also raises the fundamental question:
  can non-adaptive methods like SGD enjoy similar benefits? In this paper, we provide
  an affirmative answer to this question by proposing to achieve both robust and memory-efficient
  training via the following general recipe: (1) modify the architecture and make
  it scale invariant, (2) train with SGD and weight decay, and optionally (3) clip
  the global gradient norm proportional to weight norm multiplied by $\sqrt{\frac{2\lambda}{\eta}}$,
  where $\eta$ is learning rate and $\lambda$ is weight decay. We show that this general
  approach is robust to rescaling of parameter and loss by proving that its convergence
  only depends logarithmically on the scale of initialization and loss, whereas the
  standard SGD might not even converge for many initializations. Following our recipe,
  we design a scale invariant version of BERT, called SIBERT, which when trained simply
  by vanilla SGD achieves performance comparable to BERT trained by adaptive methods
  like Adam on downstream tasks.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li22b
month: 0
tex_title: Robust Training of Neural Networks Using Scale Invariant Architectures
firstpage: 12656
lastpage: 12684
page: 12656-12684
order: 12656
cycles: false
bibtex_author: Li, Zhiyuan and Bhojanapalli, Srinadh and Zaheer, Manzil and Reddi,
  Sashank and Kumar, Sanjiv
author:
- given: Zhiyuan
  family: Li
- given: Srinadh
  family: Bhojanapalli
- given: Manzil
  family: Zaheer
- given: Sashank
  family: Reddi
- given: Sanjiv
  family: Kumar
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/li22b/li22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
