---
title: When Are Linear Stochastic Bandits Attackable?
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: 'We study adversarial attacks on linear stochastic bandits: by manipulating
  the rewards, an adversary aims to control the behaviour of the bandit algorithm.
  Perhaps surprisingly, we first show that some attack goals can never be achieved.
  This is in a sharp contrast to context-free stochastic bandits, and is intrinsically
  due to the correlation among arms in linear stochastic bandits. Motivated by this
  finding, this paper studies the attackability of a $k$-armed linear bandit environment.
  We first provide a complete necessity and sufficiency characterization of attackability
  based on the geometry of the armsâ€™ context vectors. We then propose a two-stage
  attack method against LinUCB and Robust Phase Elimination. The method first asserts
  whether the given environment is attackable; and if yes, it poisons the rewards
  to force the algorithm to pull a target arm linear times using only a sublinear
  cost. Numerical experiments further validate the effectiveness and cost-efficiency
  of the proposed attack method.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang22ai
month: 0
tex_title: When Are Linear Stochastic Bandits Attackable?
firstpage: 23254
lastpage: 23273
page: 23254-23273
order: 23254
cycles: false
bibtex_author: Wang, Huazheng and Xu, Haifeng and Wang, Hongning
author:
- given: Huazheng
  family: Wang
- given: Haifeng
  family: Xu
- given: Hongning
  family: Wang
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/wang22ai/wang22ai.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
