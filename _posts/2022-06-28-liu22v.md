---
title: 'GACT: Activation Compressed Training for Generic Network Architectures'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Training large neural network (NN) models requires extensive memory resources,
  and Activation Compression Training (ACT) is a promising approach to reduce training
  memory footprint. This paper presents GACT, an ACT framework to support a broad
  range of machine learning tasks for generic NN architectures with limited domain
  knowledge. By analyzing a linearized version of ACTâ€™s approximate gradient, we prove
  the convergence of GACT without prior knowledge on operator type or model architecture.
  To make training stable, we propose an algorithm that decides the compression ratio
  for each tensor by estimating its impact on the gradient at run time. We implement
  GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces
  the activation memory for convolutional NNs, transformers, and graph NNs by up to
  8.1x, enabling training with a 4.2x to 24.7x larger batch size, with negligible
  accuracy loss.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu22v
month: 0
tex_title: "{GACT}: Activation Compressed Training for Generic Network Architectures"
firstpage: 14139
lastpage: 14152
page: 14139-14152
order: 14139
cycles: false
bibtex_author: Liu, Xiaoxuan and Zheng, Lianmin and Wang, Dequan and Cen, Yukuo and
  Chen, Weize and Han, Xu and Chen, Jianfei and Liu, Zhiyuan and Tang, Jie and Gonzalez,
  Joey and Mahoney, Michael and Cheung, Alvin
author:
- given: Xiaoxuan
  family: Liu
- given: Lianmin
  family: Zheng
- given: Dequan
  family: Wang
- given: Yukuo
  family: Cen
- given: Weize
  family: Chen
- given: Xu
  family: Han
- given: Jianfei
  family: Chen
- given: Zhiyuan
  family: Liu
- given: Jie
  family: Tang
- given: Joey
  family: Gonzalez
- given: Michael
  family: Mahoney
- given: Alvin
  family: Cheung
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/liu22v/liu22v.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
