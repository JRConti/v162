---
title: Do More Negative Samples Necessarily Hurt In Contrastive Learning?
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Recent investigations in noise contrastive estimation suggest, both empirically
  as well as theoretically, that while having more “negative samples” in the contrastive
  loss improves downstream classification performance initially, beyond a threshold,
  it hurts downstream performance due to a “collision-coverage” trade-off. But is
  such a phenomenon inherent in contrastive learning? We show in a simple theoretical
  setting, where positive pairs are generated by sampling from the underlying latent
  class (introduced by Saunshi et al. (ICML 2019)), that the downstream performance
  of the representation optimizing the (population) contrastive loss in fact does
  not degrade with the number of negative samples. Along the way, we give a structural
  characterization of the optimal representation in our framework, for noise contrastive
  estimation. We also provide empirical support for our theoretical results on CIFAR-10
  and CIFAR-100 datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: awasthi22b
month: 0
tex_title: Do More Negative Samples Necessarily Hurt In Contrastive Learning?
firstpage: 1101
lastpage: 1116
page: 1101-1116
order: 1101
cycles: false
bibtex_author: Awasthi, Pranjal and Dikkala, Nishanth and Kamath, Pritish
author:
- given: Pranjal
  family: Awasthi
- given: Nishanth
  family: Dikkala
- given: Pritish
  family: Kamath
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/awasthi22b/awasthi22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
