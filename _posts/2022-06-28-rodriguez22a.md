---
title: 'LyaNet: A Lyapunov Framework for Training Neural ODEs'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: We propose a method for training ordinary differential equations by using
  a control-theoretic Lyapunov condition for stability. Our approach, called LyaNet,
  is based on a novel Lyapunov loss formulation that encourages the inference dynamics
  to converge quickly to the correct prediction. Theoretically, we show that minimizing
  Lyapunov loss guarantees exponential convergence to the correct solution and enables
  a novel robustness guarantee. We also provide practical algorithms, including one
  that avoids the cost of backpropagating through a solver or using the adjoint method.
  Relative to standard Neural ODE training, we empirically find that LyaNet can offer
  improved prediction performance, faster convergence of inference dynamics, and improved
  adversarial robustness. Our code is available at https://github.com/ivandariojr/LyapunovLearning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rodriguez22a
month: 0
tex_title: "{L}ya{N}et: A {L}yapunov Framework for Training Neural {ODE}s"
firstpage: 18687
lastpage: 18703
page: 18687-18703
order: 18687
cycles: false
bibtex_author: Rodriguez, Ivan Dario Jimenez and Ames, Aaron and Yue, Yisong
author:
- given: Ivan Dario Jimenez
  family: Rodriguez
- given: Aaron
  family: Ames
- given: Yisong
  family: Yue
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/rodriguez22a/rodriguez22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
