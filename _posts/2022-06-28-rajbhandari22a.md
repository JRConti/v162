---
title: 'DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power
  Next-Generation AI Scale'
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: As the training of giant dense models hits the boundary on the availability
  and capability of the hardware resources today, Mixture-of-Experts (MoE) models
  have become one of the most promising model architectures due to their significant
  training cost reduction compared to quality-equivalent dense models. Their training
  cost saving is demonstrated from encoder-decoder models (prior works) to a 5x saving
  for auto-aggressive language models (this work). However, due to the much larger
  model size and unique architecture, how to provide fast MoE model inference remains
  challenging and unsolved, limiting their practical usage. To tackle this, we present
  DeepSpeed-MoE, an end-to-end MoE training and inference solution, including novel
  MoE architecture designs and model compression techniques that reduce MoE model
  size by up to 3.7x, and a highly optimized inference system that provides 7.3x better
  latency and cost compared to existing MoE inference solutions. DeepSpeed-MoE offers
  an unprecedented scale and efficiency to serve massive MoE models with up to 4.5x
  faster and 9x cheaper inference compared to quality-equivalent dense models. We
  hope our innovations and systems help open a promising path to new directions in
  the large model landscape, a shift from dense to sparse MoE models, where training
  and deploying higher-quality models with fewer resources becomes more widely possible.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rajbhandari22a
month: 0
tex_title: "{D}eep{S}peed-{M}o{E}: Advancing Mixture-of-Experts Inference and Training
  to Power Next-Generation {AI} Scale"
firstpage: 18332
lastpage: 18346
page: 18332-18346
order: 18332
cycles: false
bibtex_author: Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia
  and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong
author:
- given: Samyam
  family: Rajbhandari
- given: Conglong
  family: Li
- given: Zhewei
  family: Yao
- given: Minjia
  family: Zhang
- given: Reza Yazdani
  family: Aminabadi
- given: Ammar Ahmad
  family: Awan
- given: Jeff
  family: Rasley
- given: Yuxiong
  family: He
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
