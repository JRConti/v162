---
title: Revisiting and Advancing Fast Adversarial Training Through The Lens of Bi-Level
  Optimization
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: Adversarial training (AT) is a widely recognized defense mechanism to gain
  the robustness of deep neural networks against adversarial attacks. It is built
  on min-max optimization (MMO), where the minimizer (i.e., defender) seeks a robust
  model to minimize the worst-case training loss in the presence of adversarial examples
  crafted by the maximizer (i.e., attacker). However, the conventional MMO method
  makes AT hard to scale. Thus, Fast-AT and other recent algorithms attempt to simplify
  MMO by replacing its maximization step with the single gradient sign-based attack
  generation step. Although easy to implement, FAST-AT lacks theoretical guarantees,
  and its empirical performance is unsatisfactory due to the issue of robust catastrophic
  overfitting when training with strong adversaries. In this paper, we advance Fast-AT
  from the fresh perspective of bi-level optimization (BLO). We first show that the
  commonly-used Fast-AT is equivalent to using a stochastic gradient algorithm to
  solve a linearized BLO problem involving a sign operation. However, the discrete
  nature of the sign operation makes it difficult to understand the algorithm performance.
  Inspired by BLO, we design and analyze a new set of robust training algorithms termed
  Fast Bi-level AT (Fast-BAT), which effectively defends sign-based projected gradient
  descent (PGD) attacks without using any gradient sign method or explicit robust
  regularization. In practice, we show that our method yields substantial robustness
  improvements over multiple baselines across multiple models and datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang22ak
month: 0
tex_title: Revisiting and Advancing Fast Adversarial Training Through The Lens of
  Bi-Level Optimization
firstpage: 26693
lastpage: 26712
page: 26693-26712
order: 26693
cycles: false
bibtex_author: Zhang, Yihua and Zhang, Guanhua and Khanduri, Prashant and Hong, Mingyi
  and Chang, Shiyu and Liu, Sijia
author:
- given: Yihua
  family: Zhang
- given: Guanhua
  family: Zhang
- given: Prashant
  family: Khanduri
- given: Mingyi
  family: Hong
- given: Shiyu
  family: Chang
- given: Sijia
  family: Liu
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/zhang22ak/zhang22ak.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
